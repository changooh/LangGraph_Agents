{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d306f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "e:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Tuple, Literal, Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "import gradio as gr\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "\n",
    "proxy_client = get_proxy_client(\"gen-ai-hub\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042bd81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Core client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "aicore_home = os.getenv(\"AICORE_HOME\")\n",
    "profile = os.getenv(\"AICORE_PROFILE\", \"default\")\n",
    "config_path = os.path.join(aicore_home, f\"config.json\")\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "\n",
    "# 구성 파일에서 값 읽어오기\n",
    "with open(config_path, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# 클라이언트 초기화\n",
    "ai_core_client = AICoreV2Client(\n",
    "    base_url=config[\"AICORE_BASE_URL\"],\n",
    "    auth_url=config[\"AICORE_AUTH_URL\"],\n",
    "    client_id=config[\"AICORE_CLIENT_ID\"],\n",
    "    client_secret=config[\"AICORE_CLIENT_SECRET\"],\n",
    ")\n",
    "\n",
    "print(\"AI Core client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dba680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Assume OPENAI_API_KEY is set\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# This creates a RunnableSequence instance\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e6843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"cats\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7cf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Example setup (replace with actual runnables)\n",
    "runnable1 = ChatPromptTemplate.from_template(\"Runnable 1: {input}\") | model\n",
    "runnable2 = ChatPromptTemplate.from_template(\"Runnable 2: {input}\") | model\n",
    "\n",
    "# Define parallel execution using a dictionary\n",
    "parallel_chain = RunnableParallel(\n",
    "    steps={\n",
    "        \"output1\": runnable1,\n",
    "        \"output2\": runnable2,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Or equivalently:\n",
    "# parallel_chain = {\"output1\": runnable1, \"output2\": runnable2}\n",
    "\n",
    "# Invoking this runs runnable1 and runnable2 potentially in parallel\n",
    "# The input is passed to *both* runnables\n",
    "output = parallel_chain.invoke({\"input\": \"parallel processing\"})\n",
    "\n",
    "# Output will be a dictionary:\n",
    "# {'output1': AIMessage(...), 'output2': AIMessage(...)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c57e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': {'output1': AIMessage(content='Parallel processing is a method of executing multiple tasks simultaneously, speeding up data processing and computation. This can be achieved by splitting a large task into smaller sub-tasks and executing them in parallel on multiple processors or cores. \\n\\nIn this runnable example, I will demonstrate parallel processing using the Java programming language. We will create a simple program that calculates the square of numbers in parallel.\\n\\n```java\\nimport java.util.concurrent.ExecutorService;\\nimport java.util.concurrent.Executors;\\n\\npublic class ParallelProcessingExample {\\n\\n    public static void main(String[] args) {\\n        int[] numbers = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\\n\\n        ExecutorService executor = Executors.newFixedThreadPool(5);\\n\\n        for (int number : numbers) {\\n            executor.submit(() -> {\\n                int square = number * number;\\n                System.out.println(\"Square of \" + number + \" is: \" + square);\\n            });\\n        }\\n\\n        executor.shutdown();\\n    }\\n}\\n```\\n\\nIn this program, we create an array of numbers and create a fixed thread pool with 5 threads. We then iterate through the array of numbers and submit each task to the thread pool to calculate and print the square of each number.\\n\\nWhen you run this program, you will see that the squares of the numbers are calculated and printed in parallel, leveraging the multi-threading capability to speed up the processing.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 13, 'total_tokens': 303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrDNZhRY5S5VhzVJK1GlMTPgyhWjV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4c348088-754d-44c9-b308-ffa820e4bdee-0', usage_metadata={'input_tokens': 13, 'output_tokens': 290, 'total_tokens': 303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "           'output2': AIMessage(content='Parallel processing is the ability to perform multiple tasks simultaneously by splitting them into smaller subtasks and executing them concurrently on multiple processing units or cores. This allows for faster computation and improved efficiency in handling large amounts of data.\\n\\nIn order to implement parallel processing, the tasks need to be independent from each other so they can be executed concurrently without any dependencies. The process involves dividing the tasks into smaller chunks, distributing them among different processing units, and then merging the results back together. \\n\\nSome common techniques for parallel processing include multi-threading, multiprocessing, and distributed computing. These techniques leverage the power of modern computer systems with multiple cores or processors to speed up the overall processing time.\\n\\nOverall, parallel processing is a powerful tool for improving performance and efficiency in computing tasks that can be divided into smaller, independent subtasks. By leveraging the capabilities of multiple processing units, parallel processing allows for faster execution times and improved resource utilization.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 13, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrDNYE6TZnoRefu7qc2Y4JvhAxA7a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--95e33820-f253-4221-9736-d5aa3e49239e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 182, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1995adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Parallel processing is a method of executing multiple tasks simultaneously, '\n",
      " 'speeding up data processing and computation. This can be achieved by '\n",
      " 'splitting a large task into smaller sub-tasks and executing them in parallel '\n",
      " 'on multiple processors or cores. \\n'\n",
      " '\\n'\n",
      " 'In this runnable example, I will demonstrate parallel processing using the '\n",
      " 'Java programming language. We will create a simple program that calculates '\n",
      " 'the square of numbers in parallel.\\n'\n",
      " '\\n'\n",
      " '```java\\n'\n",
      " 'import java.util.concurrent.ExecutorService;\\n'\n",
      " 'import java.util.concurrent.Executors;\\n'\n",
      " '\\n'\n",
      " 'public class ParallelProcessingExample {\\n'\n",
      " '\\n'\n",
      " '    public static void main(String[] args) {\\n'\n",
      " '        int[] numbers = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\\n'\n",
      " '\\n'\n",
      " '        ExecutorService executor = Executors.newFixedThreadPool(5);\\n'\n",
      " '\\n'\n",
      " '        for (int number : numbers) {\\n'\n",
      " '            executor.submit(() -> {\\n'\n",
      " '                int square = number * number;\\n'\n",
      " '                System.out.println(\"Square of \" + number + \" is: \" + '\n",
      " 'square);\\n'\n",
      " '            });\\n'\n",
      " '        }\\n'\n",
      " '\\n'\n",
      " '        executor.shutdown();\\n'\n",
      " '    }\\n'\n",
      " '}\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'In this program, we create an array of numbers and create a fixed thread '\n",
      " 'pool with 5 threads. We then iterate through the array of numbers and submit '\n",
      " 'each task to the thread pool to calculate and print the square of each '\n",
      " 'number.\\n'\n",
      " '\\n'\n",
      " 'When you run this program, you will see that the squares of the numbers are '\n",
      " 'calculated and printed in parallel, leveraging the multi-threading '\n",
      " 'capability to speed up the processing.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output[\"steps\"][\"output1\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90907b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of passing configuration\n",
    "result = chain.with_config({\"run_name\": \"JokeGenerationRun\"}).invoke(\n",
    "    {\"topic\": \"robots\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecc7a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52d3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Why did the robot go on a diet? Because it had too many bytes!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 13, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrDO8648dDD9qoqqhoibzdVIoj7js', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2a60af9-35cc-4027-a3f1-463e3e85aa0e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 16, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "pprint(result)  # Should print \"JokeGenerationRun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822e3303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "# Synchronous Example\n",
    "# Assume setup for prompt, model, parser is done\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "sync_chain = prompt | model | output_parser\n",
    "\n",
    "result = sync_chain.invoke({\"topic\": \"programmers\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041eaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_async_chain()\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Start the asyncio event loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Assume setup for prompt, model, parser is done (as above)\n",
    "# sync_chain = prompt | model | output_parser # LCEL chains automatically support async if components do\n",
    "\n",
    "\n",
    "async def run_async_chain():\n",
    "    print(\"Running async chain...\")\n",
    "    # Use ainvoke instead of invoke\n",
    "    result = await sync_chain.ainvoke({\"topic\": \"data scientists\"})\n",
    "    print(result)\n",
    "    # If model supports streaming:\n",
    "    # print(\"Streaming response:\")\n",
    "    # async for chunk in sync_chain.astream({\"topic\": \"async programming\"}):\n",
    "    #   print(chunk, end=\"\", flush=True)\n",
    "    # print()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await run_async_chain()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start the asyncio event loop\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello, Alice! How was your trip to the store?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 277, 'total_tokens': 289, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrE90HD5kmHWjWKhTIaCk9ZyRpGXd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c5e9f3bf-f6a1-4124-b49f-62908337d00d-0' usage_metadata={'input_tokens': 277, 'output_tokens': 12, 'total_tokens': 289, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Assume 'llm' is an initialized ChatOpenAI instance\n",
    "llm = ChatOpenAI()\n",
    "# Step 1: Initial processing, maybe extract entities\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Extract names from: {input}\")\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "# Step 2: Use extracted names (state) along with original input for next step\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Generate a greeting for {name} based on this context: {original_input}\"\n",
    ")\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "# Combine, passing original input and adding 'name' to the state dict\n",
    "# The input to this chain is expected to be a dictionary, e.g., {\"input\": \"John Doe visited Paris.\"}\n",
    "complex_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        name=chain1,  # Runs chain1, adds result under 'name' key\n",
    "        original_input=lambda x: x[\"input\"],  # Passes 'input' key as 'original_input'\n",
    "    )\n",
    "    | chain2\n",
    ")  # chain2 now receives {'name': 'John Doe', 'original_input': '...'}\n",
    "\n",
    "# Example invocation:\n",
    "result = complex_chain.invoke({\"input\": \"Alice went to the store.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eede1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Step 1a: Extract topic\n",
    "prompt_topic = ChatPromptTemplate.from_template(\"What is the topic of: {input}?\")\n",
    "chain_topic = prompt_topic | llm\n",
    "\n",
    "# Step 1b: Extract sentiment\n",
    "prompt_sentiment = ChatPromptTemplate.from_template(\n",
    "    \"What is the sentiment of: {input}?\"\n",
    ")\n",
    "chain_sentiment = prompt_sentiment | llm\n",
    "\n",
    "# Step 2: Summarize using topic and sentiment\n",
    "prompt_summary = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this text: {original_input}\\nFocusing on the topic: {topic}\\nAdopt a {sentiment} tone.\"\n",
    ")\n",
    "chain_summary = prompt_summary | llm\n",
    "\n",
    "# Combine using RunnableParallel to create a state dictionary\n",
    "state_creation = RunnableParallel(\n",
    "    topic=chain_topic,\n",
    "    sentiment=chain_sentiment,\n",
    "    original_input=RunnablePassthrough(),  # Pass the original input through\n",
    ")\n",
    "\n",
    "full_chain = state_creation | chain_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44709a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='The text describes a successful product launch that exceeded expectations, with a positive sentiment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 578, 'total_tokens': 594, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrELsOoyMWgMENEEWUMtGop2tywAu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fa33857f-def0-4b9b-8400-23d6e0071b61-0', usage_metadata={'input_tokens': 578, 'output_tokens': 16, 'total_tokens': 594, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "# Example invocation:\n",
    "input_text = \"The new product launch was a huge success, exceeding all expectations.\"\n",
    "result = full_chain.invoke({\"input\": input_text})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b22ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2718e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_conversation = \"\"\"From: 김철수 (chulsoo.kim@bikecorporation.me)\n",
    "To: 이은채 (eunchae@teddyinternational.me)\n",
    "Subject: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
    "\n",
    "안녕하세요, 이은채 대리님,\n",
    "\n",
    "저는 바이크코퍼레이션의 김철수 상무입니다. 최근 보도자료를 통해 귀사의 신규 자전거 \"ZENESIS\"에 대해 알게 되었습니다. 바이크코퍼레이션은 자전거 제조 및 유통 분야에서 혁신과 품질을 선도하는 기업으로, 이 분야에서의 장기적인 경험과 전문성을 가지고 있습니다.\n",
    "\n",
    "ZENESIS 모델에 대한 상세한 브로슈어를 요청드립니다. 특히 기술 사양, 배터리 성능, 그리고 디자인 측면에 대한 정보가 필요합니다. 이를 통해 저희가 제안할 유통 전략과 마케팅 계획을 보다 구체화할 수 있을 것입니다.\n",
    "\n",
    "또한, 협력 가능성을 더 깊이 논의하기 위해 다음 주 화요일(1월 15일) 오전 10시에 미팅을 제안합니다. 귀사 사무실에서 만나 이야기를 나눌 수 있을까요?\n",
    "\n",
    "감사합니다.\n",
    "\n",
    "김철수\n",
    "상무이사\n",
    "바이크코퍼레이션\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9ed9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"다음의 이메일 내용중 중요한 내용을 추출해 주세요.\\n\\n{email_conversation}\"\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"email_conversation\": email_conversation})\n",
    "\n",
    "# output = stream_response(answer, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e151c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중요 내용 요약:\n",
      "\n",
      "- 발신자: 김철수 (바이크코퍼레이션 상무)\n",
      "- 수신자: 이은채 (Teddy International)\n",
      "- 주제: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
      "- 요청 사항: ZENESIS 모델에 대한 상세 브로슈어 (기술 사양, 배터리 성능, 디자인 정보)\n",
      "- 미팅 제안: 다음 주 화요일(1월 15일) 오전 10시, 귀사 사무실에서 만남 제안\n",
      "- 목적: 협력 가능성 논의 및 유통 전략, 마케팅 계획 구체화\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19fa9630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"person\": {\"description\": \"메일을 보낸 사람\", \"title\": \"Person\", \"type\": \"string\"}, \"email\": {\"description\": \"메일을 보낸 사람의 이메일 주소\", \"title\": \"Email\", \"type\": \"string\"}, \"subject\": {\"description\": \"메일 제목\", \"title\": \"Subject\", \"type\": \"string\"}, \"summary\": {\"description\": \"메일 본문을 요약한 텍스트\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"메일 본문에 언급된 미팅 날짜와 시간\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"person\", \"email\", \"subject\", \"summary\", \"date\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class EmailSummary(BaseModel):\n",
    "    person: str = Field(description=\"메일을 보낸 사람\")\n",
    "    email: str = Field(description=\"메일을 보낸 사람의 이메일 주소\")\n",
    "    subject: str = Field(description=\"메일 제목\")\n",
    "    summary: str = Field(description=\"메일 본문을 요약한 텍스트\")\n",
    "    date: str = Field(description=\"메일 본문에 언급된 미팅 날짜와 시간\")\n",
    "\n",
    "\n",
    "# PydanticOutputParser 생성\n",
    "parser = PydanticOutputParser(pydantic_object=EmailSummary)\n",
    "\n",
    "# instruction 을 출력합니다.\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4923c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant. Please answer the following questions in KOREAN.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EMAIL CONVERSATION:\n",
    "{email_conversation}\n",
    "\n",
    "FORMAT:\n",
    "{format}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# format 에 PydanticOutputParser의 부분 포맷팅(partial) 추가\n",
    "prompt = prompt.partial(format=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e584a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='```json\\n{\\n  \"person\": \"김철수\",\\n  \"email\": \"chulsoo.kim@bikecorporation.me\",\\n  \"subject\": \"\\\\\"ZENESIS\\\\\" 자전거 유통 협력 및 미팅 일정 제안\",\\n  \"summary\": \"김철수 상무가 이은채 대리님에게 바이크코퍼레이션의 자전거 \\'ZENESIS\\'에 대한 브로슈어 요청과 협력 논의를 위한 미팅 제안을 보냈습니다.\",\\n  \"date\": \"1월 15일 오전 10시\"\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 601, 'total_tokens': 722, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BrEsGNPPhDUMDtdqe5owNXsCmQFnc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--afe5d0b4-73b9-478e-a021-2cd0e4f61d2b-0', usage_metadata={'input_tokens': 601, 'output_tokens': 121, 'total_tokens': 722, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "# chain 을 생성합니다.\n",
    "chain = prompt | llm\n",
    "\n",
    "# chain 을 실행하고 결과를 출력합니다.\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"email_conversation\": email_conversation,\n",
    "        \"question\": \"이메일 내용중 주요 내용을 추출해 주세요.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 결과는 JSON 형태로 출력됩니다.\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2bbe665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('```json\\n'\n",
      " '{\\n'\n",
      " '  \"person\": \"김철수\",\\n'\n",
      " '  \"email\": \"chulsoo.kim@bikecorporation.me\",\\n'\n",
      " '  \"subject\": \"\\\\\"ZENESIS\\\\\" 자전거 유통 협력 및 미팅 일정 제안\",\\n'\n",
      " '  \"summary\": \"김철수 상무가 이은채 대리님에게 바이크코퍼레이션의 자전거 \\'ZENESIS\\'에 대한 브로슈어 요청과 협력 '\n",
      " '논의를 위한 미팅 제안을 보냈습니다.\",\\n'\n",
      " '  \"date\": \"1월 15일 오전 10시\"\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "# 결과는 JSON 형태로 출력됩니다.\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db5c17fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person='김철수' email='chulsoo.kim@bikecorporation.me' subject='\"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안' summary=\"김철수 상무가 이은채 대리님에게 바이크코퍼레이션의 자전거 'ZENESIS'에 대한 브로슈어 요청과 협력 논의를 위한 미팅 제안을 보냈습니다.\" date='1월 15일 오전 10시'\n"
     ]
    }
   ],
   "source": [
    "# PydanticOutputParser 를 사용하여 결과를 파싱합니다.\n",
    "structured_output = parser.parse(response.content)\n",
    "print(structured_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
