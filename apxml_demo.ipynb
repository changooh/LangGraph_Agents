{
 "cells": [
  {
   "cell_type": "code",
   "id": "7d306f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:09:19.970567Z",
     "start_time": "2025-07-10T02:08:53.891739Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Tuple, Literal, Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.summarize.map_reduce_prompt import prompt_template\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, ToolMessage\n",
    "# from langchain_core.pydantic_v1 import BaseModel\n",
    "from pydantic import BaseModel\n",
    "# from langgraph.graph import StateGraph, END, START\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "# import gradio as gr\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "\n",
    "proxy_client = get_proxy_client(\"gen-ai-hub\")\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "042bd81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:10:14.392234Z",
     "start_time": "2025-07-10T02:10:14.376339Z"
    }
   },
   "source": [
    "aicore_home = os.getenv(\"AICORE_HOME\")\n",
    "profile = os.getenv(\"AICORE_PROFILE\", \"default\")\n",
    "config_path = os.path.join(aicore_home, f\"config.json\")\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "\n",
    "# 구성 파일에서 값 읽어오기\n",
    "with open(config_path, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# 클라이언트 초기화\n",
    "ai_core_client = AICoreV2Client(\n",
    "    base_url=config[\"AICORE_BASE_URL\"],\n",
    "    auth_url=config[\"AICORE_AUTH_URL\"],\n",
    "    client_id=config[\"AICORE_CLIENT_ID\"],\n",
    "    client_secret=config[\"AICORE_CLIENT_SECRET\"],\n",
    ")\n",
    "\n",
    "print(\"AI Core client initialized successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Core client initialized successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:10:18.413459Z",
     "start_time": "2025-07-10T02:10:16.787987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(proxy_model_name='gpt-4o-mini', proxy_client=proxy_client)\n",
    "model = ChatOpenAI(proxy_model_name='gpt-4o-mini', proxy_client=proxy_client)"
   ],
   "id": "210556265f8a9241",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dba680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Assume OPENAI_API_KEY is set\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# This creates a RunnableSequence instance\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e6843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"cats\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7cf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Example setup (replace with actual runnables)\n",
    "runnable1 = ChatPromptTemplate.from_template(\"Runnable 1: {input}\") | model\n",
    "runnable2 = ChatPromptTemplate.from_template(\"Runnable 2: {input}\") | model\n",
    "\n",
    "# Define parallel execution using a dictionary\n",
    "parallel_chain = RunnableParallel(\n",
    "    steps={\n",
    "        \"output1\": runnable1,\n",
    "        \"output2\": runnable2,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Or equivalently:\n",
    "# parallel_chain = {\"output1\": runnable1, \"output2\": runnable2}\n",
    "\n",
    "# Invoking this runs runnable1 and runnable2 potentially in parallel\n",
    "# The input is passed to *both* runnables\n",
    "output = parallel_chain.invoke({\"input\": \"parallel processing\"})\n",
    "\n",
    "# Output will be a dictionary:\n",
    "# {'output1': AIMessage(...), 'output2': AIMessage(...)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c57e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': {'output1': AIMessage(content='Parallel processing is a method of executing multiple tasks simultaneously, speeding up data processing and computation. This can be achieved by splitting a large task into smaller sub-tasks and executing them in parallel on multiple processors or cores. \\n\\nIn this runnable example, I will demonstrate parallel processing using the Java programming language. We will create a simple program that calculates the square of numbers in parallel.\\n\\n```java\\nimport java.util.concurrent.ExecutorService;\\nimport java.util.concurrent.Executors;\\n\\npublic class ParallelProcessingExample {\\n\\n    public static void main(String[] args) {\\n        int[] numbers = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\\n\\n        ExecutorService executor = Executors.newFixedThreadPool(5);\\n\\n        for (int number : numbers) {\\n            executor.submit(() -> {\\n                int square = number * number;\\n                System.out.println(\"Square of \" + number + \" is: \" + square);\\n            });\\n        }\\n\\n        executor.shutdown();\\n    }\\n}\\n```\\n\\nIn this program, we create an array of numbers and create a fixed thread pool with 5 threads. We then iterate through the array of numbers and submit each task to the thread pool to calculate and print the square of each number.\\n\\nWhen you run this program, you will see that the squares of the numbers are calculated and printed in parallel, leveraging the multi-threading capability to speed up the processing.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 13, 'total_tokens': 303, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrDNZhRY5S5VhzVJK1GlMTPgyhWjV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4c348088-754d-44c9-b308-ffa820e4bdee-0', usage_metadata={'input_tokens': 13, 'output_tokens': 290, 'total_tokens': 303, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "           'output2': AIMessage(content='Parallel processing is the ability to perform multiple tasks simultaneously by splitting them into smaller subtasks and executing them concurrently on multiple processing units or cores. This allows for faster computation and improved efficiency in handling large amounts of data.\\n\\nIn order to implement parallel processing, the tasks need to be independent from each other so they can be executed concurrently without any dependencies. The process involves dividing the tasks into smaller chunks, distributing them among different processing units, and then merging the results back together. \\n\\nSome common techniques for parallel processing include multi-threading, multiprocessing, and distributed computing. These techniques leverage the power of modern computer systems with multiple cores or processors to speed up the overall processing time.\\n\\nOverall, parallel processing is a powerful tool for improving performance and efficiency in computing tasks that can be divided into smaller, independent subtasks. By leveraging the capabilities of multiple processing units, parallel processing allows for faster execution times and improved resource utilization.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 13, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrDNYE6TZnoRefu7qc2Y4JvhAxA7a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--95e33820-f253-4221-9736-d5aa3e49239e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 182, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1995adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Parallel processing is a method of executing multiple tasks simultaneously, '\n",
      " 'speeding up data processing and computation. This can be achieved by '\n",
      " 'splitting a large task into smaller sub-tasks and executing them in parallel '\n",
      " 'on multiple processors or cores. \\n'\n",
      " '\\n'\n",
      " 'In this runnable example, I will demonstrate parallel processing using the '\n",
      " 'Java programming language. We will create a simple program that calculates '\n",
      " 'the square of numbers in parallel.\\n'\n",
      " '\\n'\n",
      " '```java\\n'\n",
      " 'import java.util.concurrent.ExecutorService;\\n'\n",
      " 'import java.util.concurrent.Executors;\\n'\n",
      " '\\n'\n",
      " 'public class ParallelProcessingExample {\\n'\n",
      " '\\n'\n",
      " '    public static void main(String[] args) {\\n'\n",
      " '        int[] numbers = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\\n'\n",
      " '\\n'\n",
      " '        ExecutorService executor = Executors.newFixedThreadPool(5);\\n'\n",
      " '\\n'\n",
      " '        for (int number : numbers) {\\n'\n",
      " '            executor.submit(() -> {\\n'\n",
      " '                int square = number * number;\\n'\n",
      " '                System.out.println(\"Square of \" + number + \" is: \" + '\n",
      " 'square);\\n'\n",
      " '            });\\n'\n",
      " '        }\\n'\n",
      " '\\n'\n",
      " '        executor.shutdown();\\n'\n",
      " '    }\\n'\n",
      " '}\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'In this program, we create an array of numbers and create a fixed thread '\n",
      " 'pool with 5 threads. We then iterate through the array of numbers and submit '\n",
      " 'each task to the thread pool to calculate and print the square of each '\n",
      " 'number.\\n'\n",
      " '\\n'\n",
      " 'When you run this program, you will see that the squares of the numbers are '\n",
      " 'calculated and printed in parallel, leveraging the multi-threading '\n",
      " 'capability to speed up the processing.')\n"
     ]
    }
   ],
   "source": [
    "pprint(output[\"steps\"][\"output1\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90907b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of passing configuration\n",
    "result = chain.with_config({\"run_name\": \"JokeGenerationRun\"}).invoke(\n",
    "    {\"topic\": \"robots\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecc7a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52d3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='Why did the robot go on a diet? Because it had too many bytes!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 13, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrDO8648dDD9qoqqhoibzdVIoj7js', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2a60af9-35cc-4027-a3f1-463e3e85aa0e-0', usage_metadata={'input_tokens': 13, 'output_tokens': 16, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "pprint(result)  # Should print \"JokeGenerationRun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "822e3303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode? Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "# Synchronous Example\n",
    "# Assume setup for prompt, model, parser is done\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "sync_chain = prompt | model | output_parser\n",
    "\n",
    "result = sync_chain.invoke({\"topic\": \"programmers\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041eaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     18\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m run_async_chain()\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# Start the asyncio event loop\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     \u001B[43masyncio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py:186\u001B[39m, in \u001B[36mrun\u001B[39m\u001B[34m(main, debug)\u001B[39m\n\u001B[32m    161\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[32m    162\u001B[39m \n\u001B[32m    163\u001B[39m \u001B[33;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    182\u001B[39m \u001B[33;03m    asyncio.run(main())\u001B[39;00m\n\u001B[32m    183\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m events._get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    185\u001B[39m     \u001B[38;5;66;03m# fail fast with short traceback\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m186\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    187\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Runner(debug=debug) \u001B[38;5;28;01mas\u001B[39;00m runner:\n\u001B[32m    190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m runner.run(main)\n",
      "\u001B[31mRuntimeError\u001B[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Assume setup for prompt, model, parser is done (as above)\n",
    "# sync_chain = prompt | model | output_parser # LCEL chains automatically support async if components do\n",
    "\n",
    "\n",
    "async def run_async_chain():\n",
    "    print(\"Running async chain...\")\n",
    "    # Use ainvoke instead of invoke\n",
    "    result = await sync_chain.ainvoke({\"topic\": \"data scientists\"})\n",
    "    print(result)\n",
    "    # If model supports streaming:\n",
    "    # print(\"Streaming response:\")\n",
    "    # async for chunk in sync_chain.astream({\"topic\": \"async programming\"}):\n",
    "    #   print(chunk, end=\"\", flush=True)\n",
    "    # print()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    await run_async_chain()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start the asyncio event loop\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello, Alice! How was your trip to the store?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 277, 'total_tokens': 289, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrE90HD5kmHWjWKhTIaCk9ZyRpGXd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c5e9f3bf-f6a1-4124-b49f-62908337d00d-0' usage_metadata={'input_tokens': 277, 'output_tokens': 12, 'total_tokens': 289, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Assume 'llm' is an initialized ChatOpenAI instance\n",
    "llm = ChatOpenAI()\n",
    "# Step 1: Initial processing, maybe extract entities\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Extract names from: {input}\")\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "# Step 2: Use extracted names (state) along with original input for next step\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"Generate a greeting for {name} based on this context: {original_input}\"\n",
    ")\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "# Combine, passing original input and adding 'name' to the state dict\n",
    "# The input to this chain is expected to be a dictionary, e.g., {\"input\": \"John Doe visited Paris.\"}\n",
    "complex_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        name=chain1,  # Runs chain1, adds result under 'name' key\n",
    "        original_input=lambda x: x[\"input\"],  # Passes 'input' key as 'original_input'\n",
    "    )\n",
    "    | chain2\n",
    ")  # chain2 now receives {'name': 'John Doe', 'original_input': '...'}\n",
    "\n",
    "# Example invocation:\n",
    "result = complex_chain.invoke({\"input\": \"Alice went to the store.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eede1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Step 1a: Extract topic\n",
    "prompt_topic = ChatPromptTemplate.from_template(\"What is the topic of: {input}?\")\n",
    "chain_topic = prompt_topic | llm\n",
    "\n",
    "# Step 1b: Extract sentiment\n",
    "prompt_sentiment = ChatPromptTemplate.from_template(\n",
    "    \"What is the sentiment of: {input}?\"\n",
    ")\n",
    "chain_sentiment = prompt_sentiment | llm\n",
    "\n",
    "# Step 2: Summarize using topic and sentiment\n",
    "prompt_summary = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this text: {original_input}\\nFocusing on the topic: {topic}\\nAdopt a {sentiment} tone.\"\n",
    ")\n",
    "chain_summary = prompt_summary | llm\n",
    "\n",
    "# Combine using RunnableParallel to create a state dictionary\n",
    "state_creation = RunnableParallel(\n",
    "    topic=chain_topic,\n",
    "    sentiment=chain_sentiment,\n",
    "    original_input=RunnablePassthrough(),  # Pass the original input through\n",
    ")\n",
    "\n",
    "full_chain = state_creation | chain_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44709a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='The text describes a successful product launch that exceeded expectations, with a positive sentiment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 578, 'total_tokens': 594, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BrELsOoyMWgMENEEWUMtGop2tywAu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fa33857f-def0-4b9b-8400-23d6e0071b61-0', usage_metadata={'input_tokens': 578, 'output_tokens': 16, 'total_tokens': 594, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "# Example invocation:\n",
    "input_text = \"The new product launch was a huge success, exceeding all expectations.\"\n",
    "result = full_chain.invoke({\"input\": input_text})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b22ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2718e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_conversation = \"\"\"From: 김철수 (chulsoo.kim@bikecorporation.me)\n",
    "To: 이은채 (eunchae@teddyinternational.me)\n",
    "Subject: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
    "\n",
    "안녕하세요, 이은채 대리님,\n",
    "\n",
    "저는 바이크코퍼레이션의 김철수 상무입니다. 최근 보도자료를 통해 귀사의 신규 자전거 \"ZENESIS\"에 대해 알게 되었습니다. 바이크코퍼레이션은 자전거 제조 및 유통 분야에서 혁신과 품질을 선도하는 기업으로, 이 분야에서의 장기적인 경험과 전문성을 가지고 있습니다.\n",
    "\n",
    "ZENESIS 모델에 대한 상세한 브로슈어를 요청드립니다. 특히 기술 사양, 배터리 성능, 그리고 디자인 측면에 대한 정보가 필요합니다. 이를 통해 저희가 제안할 유통 전략과 마케팅 계획을 보다 구체화할 수 있을 것입니다.\n",
    "\n",
    "또한, 협력 가능성을 더 깊이 논의하기 위해 다음 주 화요일(1월 15일) 오전 10시에 미팅을 제안합니다. 귀사 사무실에서 만나 이야기를 나눌 수 있을까요?\n",
    "\n",
    "감사합니다.\n",
    "\n",
    "김철수\n",
    "상무이사\n",
    "바이크코퍼레이션\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9ed9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"다음의 이메일 내용중 중요한 내용을 추출해 주세요.\\n\\n{email_conversation}\"\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"email_conversation\": email_conversation})\n",
    "\n",
    "# output = stream_response(answer, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e151c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중요 내용 요약:\n",
      "\n",
      "- 발신자: 김철수 (바이크코퍼레이션 상무)\n",
      "- 수신자: 이은채 (Teddy International)\n",
      "- 주제: \"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안\n",
      "- 요청 사항: ZENESIS 모델에 대한 상세 브로슈어 (기술 사양, 배터리 성능, 디자인 정보)\n",
      "- 미팅 제안: 다음 주 화요일(1월 15일) 오전 10시, 귀사 사무실에서 만남 제안\n",
      "- 목적: 협력 가능성 논의 및 유통 전략, 마케팅 계획 구체화\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19fa9630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"person\": {\"description\": \"메일을 보낸 사람\", \"title\": \"Person\", \"type\": \"string\"}, \"email\": {\"description\": \"메일을 보낸 사람의 이메일 주소\", \"title\": \"Email\", \"type\": \"string\"}, \"subject\": {\"description\": \"메일 제목\", \"title\": \"Subject\", \"type\": \"string\"}, \"summary\": {\"description\": \"메일 본문을 요약한 텍스트\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"메일 본문에 언급된 미팅 날짜와 시간\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"person\", \"email\", \"subject\", \"summary\", \"date\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "class EmailSummary(BaseModel):\n",
    "    person: str = Field(description=\"메일을 보낸 사람\")\n",
    "    email: str = Field(description=\"메일을 보낸 사람의 이메일 주소\")\n",
    "    subject: str = Field(description=\"메일 제목\")\n",
    "    summary: str = Field(description=\"메일 본문을 요약한 텍스트\")\n",
    "    date: str = Field(description=\"메일 본문에 언급된 미팅 날짜와 시간\")\n",
    "\n",
    "\n",
    "# PydanticOutputParser 생성\n",
    "parser = PydanticOutputParser(pydantic_object=EmailSummary)\n",
    "\n",
    "# instruction 을 출력합니다.\n",
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4923c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant. Please answer the following questions in KOREAN.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "EMAIL CONVERSATION:\n",
    "{email_conversation}\n",
    "\n",
    "FORMAT:\n",
    "{format}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# format 에 PydanticOutputParser의 부분 포맷팅(partial) 추가\n",
    "prompt = prompt.partial(format=parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e584a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='```json\\n{\\n  \"person\": \"김철수\",\\n  \"email\": \"chulsoo.kim@bikecorporation.me\",\\n  \"subject\": \"\\\\\"ZENESIS\\\\\" 자전거 유통 협력 및 미팅 일정 제안\",\\n  \"summary\": \"김철수 상무가 이은채 대리님에게 바이크코퍼레이션의 자전거 \\'ZENESIS\\'에 대한 브로슈어 요청과 협력 논의를 위한 미팅 제안을 보냈습니다.\",\\n  \"date\": \"1월 15일 오전 10시\"\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 601, 'total_tokens': 722, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BrEsGNPPhDUMDtdqe5owNXsCmQFnc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--afe5d0b4-73b9-478e-a021-2cd0e4f61d2b-0', usage_metadata={'input_tokens': 601, 'output_tokens': 121, 'total_tokens': 722, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "# chain 을 생성합니다.\n",
    "chain = prompt | llm\n",
    "\n",
    "# chain 을 실행하고 결과를 출력합니다.\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"email_conversation\": email_conversation,\n",
    "        \"question\": \"이메일 내용중 주요 내용을 추출해 주세요.\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 결과는 JSON 형태로 출력됩니다.\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2bbe665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('```json\\n'\n",
      " '{\\n'\n",
      " '  \"person\": \"김철수\",\\n'\n",
      " '  \"email\": \"chulsoo.kim@bikecorporation.me\",\\n'\n",
      " '  \"subject\": \"\\\\\"ZENESIS\\\\\" 자전거 유통 협력 및 미팅 일정 제안\",\\n'\n",
      " '  \"summary\": \"김철수 상무가 이은채 대리님에게 바이크코퍼레이션의 자전거 \\'ZENESIS\\'에 대한 브로슈어 요청과 협력 '\n",
      " '논의를 위한 미팅 제안을 보냈습니다.\",\\n'\n",
      " '  \"date\": \"1월 15일 오전 10시\"\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "# 결과는 JSON 형태로 출력됩니다.\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db5c17fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person='김철수' email='chulsoo.kim@bikecorporation.me' subject='\"ZENESIS\" 자전거 유통 협력 및 미팅 일정 제안' summary=\"김철수 상무가 이은채 대리님에게 바이크코퍼레이션의 자전거 'ZENESIS'에 대한 브로슈어 요청과 협력 논의를 위한 미팅 제안을 보냈습니다.\" date='1월 15일 오전 10시'\n"
     ]
    }
   ],
   "source": [
    "# PydanticOutputParser 를 사용하여 결과를 파싱합니다.\n",
    "structured_output = parser.parse(response.content)\n",
    "print(structured_output)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:11:30.531114Z",
     "start_time": "2025-07-10T02:11:30.506145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Annotated, List\n",
    "from pydantic import Field, BaseModel, ValidationError\n",
    "\n",
    "\n",
    "class Employee(BaseModel):\n",
    "    id: Annotated[int, Field(..., description=\"직원 ID\")]\n",
    "    name: Annotated[str, Field(..., min_length=3, max_length=50, description=\"이름\")]\n",
    "    age: Annotated[int, Field(gt=18, lt=65, description=\"나이 (19-64세)\")]\n",
    "    salary: Annotated[\n",
    "        float, Field(gt=0, lt=10000, description=\"연봉 (단위: 만원, 최대 10억)\")\n",
    "    ]\n",
    "    skills: Annotated[\n",
    "        List[str], Field(min_items=1, max_items=10, description=\"보유 기술 (1-10개)\")\n",
    "    ]\n",
    "\n",
    "\n",
    "# 유효한 데이터로 인스턴스 생성\n",
    "try:\n",
    "    valid_employee = Employee(\n",
    "        id=1, name=\"테디노트\", age=30, salary=1000, skills=[\"Python\", \"LangChain\"]\n",
    "    )\n",
    "    print(\"유효한 직원 데이터:\", valid_employee)\n",
    "except ValidationError as e:\n",
    "    print(\"유효성 검사 오류:\", e)\n",
    "\n",
    "# 유효하지 않은 데이터로 인스턴스 생성 시도\n",
    "try:\n",
    "    invalid_employee = Employee(\n",
    "        name=\"테디\",  # 이름이 너무 짧음\n",
    "        age=17,  # 나이가 범위를 벗어남\n",
    "        salary=20000,  # 급여가 범위를 벗어남\n",
    "        skills=\"Python\",  # 리스트가 아님\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"유효성 검사 오류:\")\n",
    "    for error in e.errors():\n",
    "        print(f\"- {error['loc'][0]}: {error['msg']}\")\n"
   ],
   "id": "473d116b2d65ae8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유효한 직원 데이터: id=1 name='테디노트' age=30 salary=1000.0 skills=['Python', 'LangChain']\n",
      "유효성 검사 오류:\n",
      "- id: Field required\n",
      "- name: String should have at least 3 characters\n",
      "- age: Input should be greater than 18\n",
      "- salary: Input should be less than 10000\n",
      "- skills: Input should be a valid list\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:19:36.846554Z",
     "start_time": "2025-07-10T02:19:36.216468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "\n",
    "class MyData(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n"
   ],
   "id": "78f6b72e599432b8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "\n",
    "class MyData(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n"
   ],
   "id": "62b8e4182eded581"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:24:33.932685Z",
     "start_time": "2025-07-10T02:24:33.918504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "# 기본 사용 예시\n",
    "msgs1 = [HumanMessage(content=\"안녕하세요?\", id=\"1\")]\n",
    "msgs2 = [AIMessage(content=\"반갑습니다~\", id=\"2\")]\n",
    "\n",
    "result1 = add_messages(msgs1, msgs2)\n",
    "print(result1)\n"
   ],
   "id": "24adba9ad0249a01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='안녕하세요?', additional_kwargs={}, response_metadata={}, id='1'), AIMessage(content='반갑습니다~', additional_kwargs={}, response_metadata={}, id='2')]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:23:08.415104Z",
     "start_time": "2025-07-10T02:23:08.404499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 동일한 ID를 가진 메시지 대체 예시\n",
    "msgs1 = [HumanMessage(content=\"안녕하세요?\", id=\"1\")]\n",
    "msgs2 = [HumanMessage(content=\"반갑습니다~\", id=\"1\")]\n",
    "\n",
    "result2 = add_messages(msgs1, msgs2)\n",
    "print(result2)\n"
   ],
   "id": "2da7f063136a99ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='반갑습니다~', additional_kwargs={}, response_metadata={}, id='1')]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:24:35.978715Z",
     "start_time": "2025-07-10T02:24:35.971798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 동일한 ID를 가진 메시지 대체 예시\n",
    "msgs1 = [HumanMessage(content=\"안녕하세요?\", id=\"1\")]\n",
    "msgs2 = [HumanMessage(content=\"반갑습니다~\", id=\"1\")]\n",
    "\n",
    "result1 = add_messages(msgs1, msgs2)\n",
    "print(result1)\n"
   ],
   "id": "198a1c6b436ba25d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='반갑습니다~', additional_kwargs={}, response_metadata={}, id='1')]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:39:30.891271Z",
     "start_time": "2025-07-10T02:39:30.880164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # 메시지 정의(list type 이며 add_messages 함수를 사용하여 메시지를 추가)\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 정의\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "# 챗봇 함수 정의\n",
    "def chatbot(state: State):\n",
    "    # 메시지 호출 및 반환\n",
    "    r_msg = AIMessage(content=\"so what?\")\n",
    "    return {\"messages\": [r_msg]}\n",
    "    # return {\"messages\": [llm.invoke(state[\"messages\"])]}\n"
   ],
   "id": "3965a475a1ac65e0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:39:33.247495Z",
     "start_time": "2025-07-10T02:39:33.017491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 그래프 생성\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 노드 이름, 함수 혹은 callable 객체를 인자로 받아 노드를 추가\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 시작 노드에서 챗봇 노드로의 엣지 추가\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "# 그래프에 엣지 추가\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "# 그래프 컴파일\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 그래프 시각화\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ],
   "id": "85d317fe956f0143",
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T05:39:19.501079Z",
     "start_time": "2025-07-10T05:39:19.487329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"안녕하세요 제이름은 테디입니다.\" \n",
    "\n",
    "# 그래프 이벤트 스트리밍\n",
    "for event in graph.stream({\"messages\": [(\"user\", question)]}):\n",
    "    # 이벤트 값 출력\n",
    "    for value in event.values():\n",
    "        print(\"Assistant:\", value[\"messages\"][-1].content)\n"
   ],
   "id": "dc689e81de0b8d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: so what?\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "859c828b8f220767"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:02:29.762121Z",
     "start_time": "2025-07-10T11:02:29.747792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import OpenAI\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class TimeCalculationTool(BaseModel):\n",
    "    query: str = Field(..., description=\"The query to summarize of the given overtime hours.\")\n"
   ],
   "id": "9c329b56901f96ad",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:11:41.401629Z",
     "start_time": "2025-07-10T11:11:41.394845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @tool(\"TimeCalculator\", args_schema=TimeCalculationTool)\n",
    "def time_calculator(query) -> str:\n",
    "    \"\"\" 사용자 근무 시간 정보를 바탕으로 보상 휴가 시간을 계산합니다. \"\"\"\n",
    "     # 프롬프트 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"Based on the following information, summarize all the working hours and calculate compensatory leave hours. \n",
    "        Follow these steps very carefully:\n",
    "        \n",
    "        1. Set fixed Over Time standards:\n",
    "           Weekday fixed Over Time = 30 hours\n",
    "           Weekend fixed Over Time = 10 hours\n",
    "        \n",
    "        2. Calculate weekday compensatory leave:\n",
    "           a. Excess weekday overtime = MAX(0, weekday_overtime - 30)\n",
    "           b-1. no exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 0.5)\n",
    "           b-2. exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 2)\n",
    "        \n",
    "        3. Calculate weekend compensatory leave:\n",
    "           a. Excess weekend overtime = MAX(0, weekend_overtime - 10)\n",
    "           b-1. If no exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 0.5) \n",
    "           b-2. If exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 2)\n",
    "        \n",
    "        4. Calculate total compensatory leave:\n",
    "           Total compensatory leave hours = Weekday compensatory leave + Weekend compensatory leave\n",
    "        \n",
    "        5. Round all calculated hours to two decimal places.\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        output format summary as follows:\n",
    "        <summary>\n",
    "        === 보상 휴가 시간 계산 결과 ===\n",
    "        평일 보상 휴가 시간: \n",
    "        주말/휴일 보상 휴가 시간: \n",
    "        총 보상 휴가 시간: \n",
    "        주말/휴일 근무 시간에 \"대체 휴일\"을 적용한 경우, 주말/휴일 보상 시간은 위의 계산 결과와 다를 수 있습니다.\n",
    "        </summary>\n",
    "        \"\"\"\n",
    "            )\n",
    "    try:\n",
    "        # 프롬프트 생성 및 LLM 실행\n",
    "        formatted_prompt = prompt_template.format(context=context)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 응답 텍스트 반환\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"대체휴가 계산 중 오류가 발생했습니다: {str(e)}\"\n",
    "    \n",
    "    "
   ],
   "id": "53a29b657e5271c4",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    llm: Optional[OpenAI] = None  # 사용할 LLM 인스턴스\n",
    "    prompt_template = \"\"\"Based on the following information, summarize all the working hours and calculate compensatory leave hours. \n",
    "        Follow these steps very carefully:\n",
    "            \n",
    "        1. Set fixed Over Time standards:\n",
    "           Weekday fixed Over Time = 30 hours\n",
    "           Weekend fixed Over Time = 10 hours\n",
    "        \n",
    "        2. Calculate weekday compensatory leave:\n",
    "           a. Excess weekday overtime = MAX(0, weekday_overtime - 30)\n",
    "           b-1. no exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 0.5)\n",
    "           b-2. exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 2)\n",
    "        \n",
    "        3. Calculate weekend compensatory leave:\n",
    "           a. Excess weekend overtime = MAX(0, weekend_overtime - 10)\n",
    "           b-1. If no exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 0.5) \n",
    "           b-2. If exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 2)\n",
    "        \n",
    "        4. Calculate total compensatory leave:\n",
    "           Total compensatory leave hours = Weekday compensatory leave + Weekend compensatory leave\n",
    "        \n",
    "        5. Round all calculated hours to two decimal places.provide a clear and concise summary of the overtime hours they want to calculate for compensatory leave.\n",
    "        \n",
    "        output format summary as follows:\n",
    "        <summary>\n",
    "        === 보상 휴가 시간 계산 결과 ===\n",
    "        평일 보상 휴가 시간: \n",
    "        주말/휴일 보상 휴가 시간: \n",
    "        총 보상 휴가 시간: \n",
    "        주말/휴일 근무 시간에 \"대체 휴일\"을 적용한 경우, 주말/휴일 보상 시간은 위의 계산 결과와 다를 수 있습니다.\n",
    "        </summary>\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    def _run(self, weekday_overtime: str, weekday_night: str, weekend_overtime: str, weekend_night: str) -> str:\n",
    "        prompt = self.prompt_template.format(\n",
    "            weekday_overtime=weekday_overtime,\n",
    "            weekday_night=weekday_night,\n",
    "            weekend_overtime=weekend_overtime,\n",
    "            weekend_night=weekend_night\n",
    "        )\n",
    "        return self.llm(prompt)\n",
    "\n",
    "# 사용 예시\n",
    "tool = SummarizeOvertimeTool(llm=llm)\n",
    "result = tool.run(\"=== 입력 정보 요약 === 평일 연장 근무 시간 : 10시간  평일 야간 근무 시간:2시간 주말 연장 근무 시간: 0시간 주말 야간 근무 시간: 2시간 \")\n",
    "print(result)\n"
   ],
   "id": "8278c976db71847f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:26:50.695391Z",
     "start_time": "2025-07-10T06:26:48.196630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Dict, Any\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def summarize_overtime_context(context: str) -> str:\n",
    "    \"\"\"\n",
    "    대화 내용을 기반으로 대체휴가 계산을 위한 연장근무 시간 정보를 요약합니다.\n",
    "    \n",
    "    Args:\n",
    "        context (str): 사용자와의 대화 내용\n",
    "    \n",
    "    Returns:\n",
    "        str: 연장근무 시간 정보가 정리된 요약문\n",
    "    \"\"\"\n",
    "    \n",
    "    # # OpenAI API 키 확인\n",
    "    # if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    #     return \"OpenAI API 키가 설정되지 않았습니다.\"\n",
    "    # \n",
    "    # # LLM 모델 초기화\n",
    "    # llm = ChatOpenAI(\n",
    "    #     model=\"gpt-3.5-turbo\",\n",
    "    #     temperature=0\n",
    "    # )\n",
    "    \n",
    "    # 프롬프트 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"\n",
    "Based on the user's input information, provide a clear and concise summary of the overtime hours they want to calculate for compensatory leave.\n",
    "Please summarize the following information in Korean:\n",
    "- 평일 연장 근무 시간 (Weekday overtime hours)\n",
    "- 평일 야간 근무 시간 (Weekday night work hours)\n",
    "- 주말 연장 시간 (Weekend overtime hours)\n",
    "- 주말 야간 근무 시간 (Weekend night work hours)\n",
    "\n",
    "Always ask \" 보상휴가를 계산해 드릴까요? \" in the end of the response.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "output format summary as follows:\n",
    "=== 입력 정보 요약 ===\n",
    "평일 연장 근무 시간 : \n",
    "평일 야간 근무 시간: \n",
    "주말 연장 근무 시간: \n",
    "주말 야간 근무 시간: \n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 프롬프트 생성 및 LLM 실행\n",
    "        formatted_prompt = prompt_template.format(context=context)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 응답 텍스트 반환\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"요약 생성 중 오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 테스트용 대화 내용\n",
    "    test_context = \"\"\"\n",
    "    사용자: 안녕하세요, 대체휴가 계산을 도와주세요.\n",
    "    어시스턴트: 네, 도와드리겠습니다. 연장근무 시간 정보를 알려주세요.\n",
    "    사용자: 저는 평일에 5시간 연장근무를 했고, 평일 야간에는 2시간 일했습니다.\n",
    "    사용자: 그리고 주말에는 8시간 연장근무를 했어요.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = summarize_overtime_context(test_context)\n",
    "    print(result)"
   ],
   "id": "f9a04dbdb3a25bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 입력 정보 요약 ===  \n",
      "평일 연장 근무 시간 : 5시간  \n",
      "평일 야간 근무 시간: 2시간  \n",
      "주말 연장 근무 시간: 8시간  \n",
      "주말 야간 근무 시간: 0시간  \n",
      "\n",
      "보상휴가를 계산해 드릴까요?\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:38:29.396873Z",
     "start_time": "2025-07-10T06:38:19.168896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Dict, Any\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def calculate_compensatory_leave(context: str) -> str:\n",
    "    \"\"\"\n",
    "    대화 내용을 기반으로 연장근무 시간을 분석하고 대체휴가 시간을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        context (str): 사용자와의 대화 내용 (연장근무 시간 정보 포함)\n",
    "    \n",
    "    Returns:\n",
    "        str: 대체휴가 시간 계산 결과\n",
    "    \"\"\"\n",
    "    \n",
    "    # 프롬프트 템플릿 정의\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"\n",
    "Based on the following information, summarize all the working hours and calculate compensatory leave hours. \n",
    "Follow these steps very carefully:\n",
    "\n",
    "1. Set fixed Over Time standards:\n",
    "   Weekday fixed Over Time = 30 hours\n",
    "   Weekend fixed Over Time = 10 hours\n",
    "\n",
    "2. Calculate weekday compensatory leave:\n",
    "   a. Excess weekday overtime = MAX(0, weekday_overtime - 30)\n",
    "   b-1. no exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 0.5)\n",
    "   b-2. exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 2)\n",
    "\n",
    "3. Calculate weekend compensatory leave:\n",
    "   a. Excess weekend overtime = MAX(0, weekend_overtime - 10)\n",
    "   b-1. If no exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 0.5) \n",
    "   b-2. If exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 2)\n",
    "\n",
    "4. Calculate total compensatory leave:\n",
    "   Total compensatory leave hours = Weekday compensatory leave + Weekend compensatory leave\n",
    "\n",
    "5. Round all calculated hours to two decimal places.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "output format summary as follows:\n",
    "<summary>\n",
    "=== 보상 휴가 시간 계산 결과 ===\n",
    "평일 보상 휴가 시간: \n",
    "주말/휴일 보상 휴가 시간: \n",
    "총 보상 휴가 시간: \n",
    "주말/휴일 근무 시간에 \"대체 휴일\"을 적용한 경우, 주말/휴일 보상 시간은 위의 계산 결과와 다를 수 있습니다.\n",
    "</summary>\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 프롬프트 생성 및 LLM 실행\n",
    "        formatted_prompt = prompt_template.format(context=context)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 응답 텍스트 반환\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"대체휴가 계산 중 오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 테스트용 대화 내용\n",
    "    test_context = \"\"\"\n",
    "    평일 연장근무: 30시간\n",
    "    평일 야간근무: 2시간\n",
    "    주말 연장근무: 10시간\n",
    "    주말 야간근무: 0시간\n",
    "    \"\"\"\n",
    "    result = calculate_compensatory_leave(test_context)\n",
    "    import re\n",
    "\n",
    "    def extract_summary(text):\n",
    "        \"\"\"\n",
    "        문자열에서 <summary> 태그 안의 텍스트를 추출합니다.\n",
    "        \n",
    "        Args:\n",
    "            text (str): 검색할 문자열\n",
    "            \n",
    "        Returns:\n",
    "            str: summary 태그 안의 텍스트 (태그가 없으면 빈 문자열)\n",
    "        \"\"\"\n",
    "        pattern = r'<summary>(.*?)</summary>'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"\"\n",
    "    \n",
    "    print(extract_summary(result))\n",
    "    "
   ],
   "id": "4938a4abba962ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 보상 휴가 시간 계산 결과 ===\n",
      "평일 보상 휴가 시간: 1.00\n",
      "주말/휴일 보상 휴가 시간: 0.00\n",
      "총 보상 휴가 시간: 1.00\n",
      "주말/휴일 근무 시간에 \"대체 휴일\"을 적용한 경우, 주말/휴일 보상 시간은 아래 계산 결과와 다를 수 있습니다.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:57:16.343972Z",
     "start_time": "2025-07-10T06:55:02.682411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, Any, List, Annotated\n",
    "import os\n",
    "\n",
    "# 사용자 스토리 기준 클래스\n",
    "class WorkHoursCriteria(BaseModel):\n",
    "    weekday_overtime: str\n",
    "    weekday_night: str\n",
    "    weekend_overtime: str\n",
    "    weekend_night: str\n",
    "\n",
    "# 이전에 정의한 툴들\n",
    "@tool\n",
    "def summarize_overtime_context(context: str) -> str:\n",
    "    \"\"\"\n",
    "    대화 내용을 기반으로 대체휴가 계산을 위한 연장근무 시간 정보를 요약합니다.\n",
    "    \n",
    "    Args:\n",
    "        context (str): 사용자와의 대화 내용\n",
    "    \n",
    "    Returns:\n",
    "        str: 연장근무 시간 정보가 정리된 요약문\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"OpenAI API 키가 설정되지 않았습니다.\"\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"\n",
    "Based on the user's input information, provide a clear and concise summary of the overtime hours they want to calculate for compensatory leave.\n",
    "Please summarize the following information in Korean:\n",
    "- 평일 연장 근무 시간 (Weekday overtime hours)\n",
    "- 평일 야간 근무 시간 (Weekday night work hours)\n",
    "- 주말 연장 시간 (Weekend overtime hours)\n",
    "- 주말 야간 근무 시간 (Weekend night work hours)\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "output format summary as follows:\n",
    "=== 입력 정보 요약 ===\n",
    "평일 연장 근무 시간 : \n",
    "평일 야간 근무 시간: \n",
    "주말 연장 근무 시간: \n",
    "주말 야간 근무 시간: \n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format(context=context)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"요약 생성 중 오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def calculate_compensatory_leave(context: str) -> str:\n",
    "    \"\"\"\n",
    "    대화 내용을 기반으로 연장근무 시간을 분석하고 대체휴가 시간을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        context (str): 사용자와의 대화 내용 (연장근무 시간 정보 포함)\n",
    "    \n",
    "    Returns:\n",
    "        str: 대체휴가 시간 계산 결과\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"OpenAI API 키가 설정되지 않았습니다.\"\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"\"\"\n",
    "Based on the following information, summarize all the working hours and calculate compensatory leave hours. \n",
    "Follow these steps very carefully:\n",
    "\n",
    "1. Set fixed Over Time standards:\n",
    "   Weekday fixed Over Time = 30 hours\n",
    "   Weekend fixed Over Time = 10 hours\n",
    "\n",
    "2. Calculate weekday compensatory leave:\n",
    "   a. Excess weekday overtime = MAX(0, weekday_overtime - 30)\n",
    "   b-1. no exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 0.5)\n",
    "   b-2. exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 2)\n",
    "\n",
    "3. Calculate weekend compensatory leave:\n",
    "   a. Excess weekend overtime = MAX(0, weekend_overtime - 10)\n",
    "   b-1. If no exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 0.5) \n",
    "   b-2. If exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 2)\n",
    "\n",
    "4. Calculate total compensatory leave:\n",
    "   Total compensatory leave hours = Weekday compensatory leave + Weekend compensatory leave\n",
    "\n",
    "5. Round all calculated hours to two decimal places.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "output format summary as follows:\n",
    "=== 보상 휴가 시간 계산 결과 ===\n",
    "평일 보상 휴가 시간: \n",
    "주말, 휴일 보상 휴가 시간: \n",
    "총 보상 휴가 시간: \n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format(context=context)\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"대체휴가 계산 중 오류가 발생했습니다: {str(e)}\"\n",
    "\n",
    "# 상태 정의\n",
    "class AgentState(BaseModel):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    collected_info: Dict[str, str] = Field(default_factory=dict)\n",
    "    context: str = \"\"\n",
    "\n",
    "# React Agent 클래스\n",
    "class CompensatoryLeaveAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "        self.tools = [summarize_overtime_context, calculate_compensatory_leave]\n",
    "        self.tool_node = ToolNode(self.tools)\n",
    "        \n",
    "        # 에이전트 프롬프트 템플릿\n",
    "        self.agent_prompt = \"\"\"You're my employee's compensatory leave hours calculation agent. Your job is to gather over time hours information from the employees about compensatory leave hours and give the summary or calculation result. \n",
    "\n",
    "        You should obtain the following information from them:\n",
    "\n",
    "        - weekday_overtime is total Over time of weekdays of a month: 월간 평일 총 연장 근무 시간. 이것은 평일 18시부터 22시 사이에 발생한 총 연장 근무 시간. 시/분 단위로 근무 시간을 수집 한다. (예) 1시간, 1시간 30분, 20분 등. \n",
    "        - weekday_night is total Night work time of weekdays of a month : 월간 평일 총 야간 근무 시간. 이것은 평일 22시부터 다음날 6시 사이에 발생한 총 야간 근무 시간. 시/분 단위로 근무 시간을 수집 한다. (예) 1시간, 1시간 30분, 20분 등.\n",
    "        - weekend_overtime is total Over time of weekends of a month: 월간 주말 총 연장 근무 시간. 이것은 주말 6시부터 22시 사이에 발생한 총 주말 연장 근무 시간. 시/분 단위로 근무 시간을 수집 한다.(예) 1시간, 1시간 30분, 20분 등. \n",
    "        - weekend_night is total Night work time of weekends of a month : 월간 주말 총 야간 근무 시간. 이것은 주말 22시부터 다음날 6시 사이에 발생한 총 주말 야간 근무 시간. 시/분 단위로 근무 시간을 수집 한다. (예) 1시간, 1시간 30분, 20분 등.\n",
    "\n",
    "        If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. \n",
    "        Whenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of compensatory Leave Hours Calculation . If not, ask questions to help the user better detail the criterion.\n",
    "        Do not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. \n",
    "        Always remind them that if they do not know how to answer something, you can help them.\n",
    "\n",
    "        After you are able to discern all the information, call the relevant tool.\n",
    "        \n",
    "        Current conversation context: {context}\n",
    "        Collected information so far: {collected_info}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.graph = self._create_graph()\n",
    "    \n",
    "    def _create_graph(self):\n",
    "        \"\"\"LangGraph 워크플로우 생성\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # 노드 추가\n",
    "        workflow.add_node(\"agent\", self.agent_node)\n",
    "        workflow.add_node(\"tools\", self.tool_node)\n",
    "        \n",
    "        # 진입점 설정\n",
    "        workflow.set_entry_point(\"agent\")\n",
    "        \n",
    "        # 조건부 엣지 추가\n",
    "        workflow.add_conditional_edges(\n",
    "            \"agent\",\n",
    "            self.should_continue,\n",
    "            {\n",
    "                \"continue\": \"tools\",\n",
    "                \"end\": END,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # 툴에서 에이전트로 돌아가는 엣지\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def agent_node(self, state: AgentState):\n",
    "        \"\"\"에이전트 노드 - 사용자와 대화하고 툴 호출 결정\"\"\"\n",
    "        # 대화 컨텍스트 업데이트\n",
    "        context = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in state.messages])\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        prompt = self.agent_prompt.format(\n",
    "            context=context,\n",
    "            collected_info=state.collected_info\n",
    "        )\n",
    "        \n",
    "        # LLM 호출\n",
    "        response = self.llm.bind_tools(self.tools).invoke([\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            *[{\"role\": msg.type, \"content\": msg.content} for msg in state.messages]\n",
    "        ])\n",
    "        \n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    def should_continue(self, state: AgentState):\n",
    "        \"\"\"툴 호출 여부 결정\"\"\"\n",
    "        last_message = state.messages[-1]\n",
    "        \n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "    \n",
    "    def chat(self, message: str):\n",
    "        \"\"\"사용자 메시지 처리\"\"\"\n",
    "        state = AgentState(messages=[HumanMessage(content=message)])\n",
    "        result = self.graph.invoke(state)\n",
    "        return result[\"messages\"][-1].content\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경 변수 설정 확인\n",
    "    # if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    #     print(\"OPENAI_API_KEY 환경 변수를 설정해주세요.\")\n",
    "    #     exit()\n",
    "    \n",
    "    # 에이전트 생성\n",
    "    agent = CompensatoryLeaveAgent()\n",
    "    \n",
    "    # 대화 시뮬레이션\n",
    "    # print(\"대체휴가 계산 에이전트입니다. 연장근무 시간 정보를 알려주세요.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n사용자: \")\n",
    "        if user_input.lower() in ['quit', 'exit', '종료']:\n",
    "            break\n",
    "        \n",
    "        response = agent.chat(user_input)\n",
    "        print(f\"에이전트: {response}\")"
   ],
   "id": "b725d9bb5708045c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에이전트: 안녕하세요! 대체휴가 시간을 계산하기 위해 필요한 정보를 수집하고 있습니다. 시작해볼까요?\n",
      "\n",
      "먼저, 월간 평일 총 연장 근무 시간(weekday_overtime)을 말씀해 주실 수 있나요? 시/분 단위로 알려주세요. 예를 들어, \"1시간\" 또는 \"1시간 30분\" 같은 형식으로요.\n",
      "에이전트: 감사합니다! 그러나 이 정보는 어떤 종류의 연장 근무 시간을 나타내는지 명확하지 않습니다. 아래의 기준 중 하나에 대한 정보를 제공해 주시겠어요?\n",
      "\n",
      "1. 평일 연장 근무 시간 (weekday_overtime)\n",
      "2. 평일 야간 근무 시간 (weekday_night)\n",
      "3. 주말 연장 근무 시간 (weekend_overtime)\n",
      "4. 주말 야간 근무 시간 (weekend_night)\n",
      "\n",
      "어떤 정보인지 명확히 알려주시면 다음 단계로 진행할 수 있습니다. 만약 대답하기 어려운 부분이 있다면 도와드릴 수 있습니다!\n",
      "에이전트: 평일 연장 근무 시간이 30시간이라고 하셨네요. 계속해서 나머지 정보를 수집하겠습니다.\n",
      "\n",
      "이제 월간 평일 총 야간 근무 시간(평일 22시부터 다음 날 6시 사이의 시간)은 얼마인지 알려주세요. 시/분 단위로 입력해 주시면 됩니다.\n",
      "에이전트: 잘 부탁드립니다! 대체휴가 계산을 위해 필요한 몇 가지 정보를 수집하고자 합니다. 먼저 평일의 연장 근무 시간부터 말씀해 주실 수 있으신가요?\n",
      "\n",
      "- **평일 연장 근무 시간 (weekday_overtime)**: 평일 18시부터 22시 사이에 발생한 총 연장 근무 시간 (예: 1시간, 1시간 30분, 20분 등).\n",
      "에이전트: 평일 야간 근무 시간으로 2시간을 확인했습니다. \n",
      "\n",
      "이제 다음 정보를 알려주시겠어요?\n",
      "\n",
      "- 평일 연장 근무 시간(weekday_overtime): 평일 18시부터 22시 사이에 발생한 총 연장 근무 시간입니다. \n",
      "- 주말 연장 근무 시간(weekend_overtime): 주말 6시부터 22시 사이에 발생한 총 주말 연장 근무 시간입니다. \n",
      "- 주말 야간 근무 시간(weekend_night): 주말 22시부터 다음날 6시 사이에 발생한 총 주말 야간 근무 시간입니다. \n",
      "\n",
      "어떤 정보를 제공해 주실 수 있나요? 도움이 필요하시면 말씀해 주세요!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[66]\u001B[39m\u001B[32m, line 234\u001B[39m\n\u001B[32m    230\u001B[39m \u001B[38;5;66;03m# 대화 시뮬레이션\u001B[39;00m\n\u001B[32m    231\u001B[39m \u001B[38;5;66;03m# print(\"대체휴가 계산 에이전트입니다. 연장근무 시간 정보를 알려주세요.\")\u001B[39;00m\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m234\u001B[39m     user_input = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[33;43m사용자: \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    235\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m user_input.lower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m'\u001B[39m\u001B[33mquit\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mexit\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m종료\u001B[39m\u001B[33m'\u001B[39m]:\n\u001B[32m    236\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LangGraph_Agents\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1280\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1281\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1282\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1283\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1284\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1285\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1286\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1287\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LangGraph_Agents\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1322\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1323\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1324\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1325\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1326\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1327\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:36:22.018746Z",
     "start_time": "2025-07-10T12:36:22.004663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ],
   "id": "97072855d3755568",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:36:27.231410Z",
     "start_time": "2025-07-10T12:36:27.222634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tool\n",
    "def summarizer(reqs: str) -> str:\n",
    "    \"\"\" use this to summarize the given overtime hours \"\"\"\n",
    "    template01 = \"\"\" provide a clear and concise summary of the overtime hours they want to calculate for compensatory leave.\n",
    "        Please summarize the following information in Korean:\n",
    "        - 평일 연장 근무 시간 (Weekday overtime hours)\n",
    "        - 평일 야간 근무 시간 (Weekday night work hours)\n",
    "        - 주말 연장 시간 (Weekend overtime hours)\n",
    "        - 주말 야간 근무 시간 (Weekend night work hours)\n",
    "        \n",
    "        Always ask \"보상 휴가 시간을 계산해 드릴까요?\" in the end of the response.\n",
    "        \n",
    "        output format summary as follows:\n",
    "        === 입력 정보 요약 ===\n",
    "        평일 연장 근무 시간 : \n",
    "        평일 야간 근무 시간: \n",
    "        주말 연장 근무 시간: \n",
    "        주말 야간 근무 시간: \n",
    "\n",
    "        {reqs}\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    template=template01\n",
    ")\n",
    "    chain = prompt | llm \n",
    "    response=chain.invoke({\"reqs\": reqs})\n",
    "    return response.content"
   ],
   "id": "d99ce66a6ed024d7",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:36:49.778991Z",
     "start_time": "2025-07-10T12:36:49.770133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@tool\n",
    "def calculator(reqs: str) -> None:\n",
    "    \"\"\" use this to calculate the given overtime hours \"\"\"\n",
    "    template02 = \"\"\" Based on the following information, summarize all the working hours and calculate compensatory leave hours. \n",
    "        Follow these steps very carefully:\n",
    "            \n",
    "        1. Set fixed Over Time standards:\n",
    "           Weekday fixed Over Time = 30 hours\n",
    "           Weekend fixed Over Time = 10 hours\n",
    "        \n",
    "        2. Calculate weekday compensatory leave:\n",
    "           a. Excess weekday overtime = MAX(0, weekday_overtime - 30)\n",
    "           b-1. no exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 0.5)\n",
    "           b-2. exceeding the weekday fixed over time, Weekday compensatory leave = (Excess weekday overtime × 1.5) + (Weekday night work × 2)\n",
    "        \n",
    "        3. Calculate weekend compensatory leave:\n",
    "           a. Excess weekend overtime = MAX(0, weekend_overtime - 10)\n",
    "           b-1. If no exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 0.5) \n",
    "           b-2. If exceeding the weekend fixed over time, Weekend compensatory leave = (excess weekend overtime × 1.5) + (Weekend night work × 2)\n",
    "        \n",
    "        4. Calculate total compensatory leave:\n",
    "           Total compensatory leave hours = Weekday compensatory leave + Weekend compensatory leave\n",
    "        \n",
    "        5. Round all calculated hours to two decimal places.provide a clear and concise summary of the overtime hours they want to calculate for compensatory leave.\n",
    "        \n",
    "        output format summary as follows:\n",
    "        <summary>\n",
    "        === 보상 휴가 시간 계산 결과 ===\n",
    "        평일 보상 휴가 시간: \n",
    "        주말/휴일 보상 휴가 시간: \n",
    "        총 보상 휴가 시간: \n",
    "        주말/휴일 근무 시간에 \"대체 휴일\"을 적용한 경우, 주말/휴일 보상 시간은 위의 계산 결과와 다를 수 있습니다.\n",
    "        </summary>\n",
    "        \n",
    "        {reqs}\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt01 = ChatPromptTemplate.from_template(\n",
    "    template=template02)\n",
    "    chain01 = prompt01 | llm \n",
    "    response=chain01.invoke({\"reqs\": reqs})\n",
    "    return response.content\n"
   ],
   "id": "167315ba09ba35c3",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:40:51.630353Z",
     "start_time": "2025-07-10T12:40:51.559051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = \"평일 연장근무는 몇시간 인가요? 10시간입니다. 평일 야간근무는 0시간 입니다. 주말 연장근무는 30분이구요. 주말 야간근무는 4시간입니다.\"\n",
    "    \n",
    "result = calculator(reqs = sample)\n"
   ],
   "id": "e8e04b44f6702523",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseTool.__call__() got an unexpected keyword argument 'reqs'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[133]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m sample = \u001B[33m\"\u001B[39m\u001B[33m평일 연장근무는 몇시간 인가요? 10시간입니다. 평일 야간근무는 0시간 입니다. 주말 연장근무는 30분이구요. 주말 야간근무는 4시간입니다.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m result = \u001B[43mcalculator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreqs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LangGraph_Agents\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    187\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    188\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: BaseTool.__call__() got an unexpected keyword argument 'reqs'"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:41:28.761660Z",
     "start_time": "2025-07-10T12:41:28.744691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tools = [summarizer,calculator]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ],
   "id": "83e848e16f0937a8",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:41:32.257002Z",
     "start_time": "2025-07-10T12:41:32.244286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# 에이전트 생성\n",
    "agent_executor = create_react_agent(llm, tools)"
   ],
   "id": "b80cdaf59310e79b",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:41:52.814825Z",
     "start_time": "2025-07-10T12:41:52.809165Z"
    }
   },
   "cell_type": "code",
   "source": "sample = \"\"\" 평일 연장근무는 몇시간 인가요? 10시간입니다. 평일 야간근무는 0시간 입니다. 주말 연장근무는 30분이구요. 주말 야간근무는 4시간입니다. \"\"\"",
   "id": "a49049ae6850d30c",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:42:13.835175Z",
     "start_time": "2025-07-10T12:41:56.592677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "human_message = [HumanMessage(content=sample)]\n",
    "\n",
    "response = agent_executor.invoke({\"messages\": human_message})\n",
    "# response[\"messages\"]"
   ],
   "id": "83926c9122dffdc3",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:42:16.179810Z",
     "start_time": "2025-07-10T12:42:16.173498Z"
    }
   },
   "cell_type": "code",
   "source": "last_message = response[\"messages\"][-1]",
   "id": "879f660dc702bbfc",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:43:29.569314Z",
     "start_time": "2025-07-10T12:43:29.557063Z"
    }
   },
   "cell_type": "code",
   "source": "from pprint import pprint",
   "id": "8b2e619f5c0882c6",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T12:43:32.894429Z",
     "start_time": "2025-07-10T12:43:32.887294Z"
    }
   },
   "cell_type": "code",
   "source": "pprint(last_message.content)",
   "id": "81b2ed6fc6dac8c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('### 요약 정보\\n'\n",
      " '\\n'\n",
      " '- 평일 연장 근무 시간: 10시간\\n'\n",
      " '- 평일 야간 근무 시간: 0시간\\n'\n",
      " '- 주말 연장 근무 시간: 30분\\n'\n",
      " '- 주말 야간 근무 시간: 4시간\\n'\n",
      " '\\n'\n",
      " '#### 총 보상 휴가 계산 결과\\n'\n",
      " '\\n'\n",
      " '1. **평일 보상 휴가**\\n'\n",
      " '   - 추가 평일 연장 근무: 0시간\\n'\n",
      " '   - 따라서, 평일 보상 휴가: **0.00시간**\\n'\n",
      " '\\n'\n",
      " '2. **주말 보상 휴가**\\n'\n",
      " '   - 추가 주말 연장 근무: 0시간\\n'\n",
      " '   - 주말 야간 근무 시간을 고려하여 보상 휴가: **2.00시간**\\n'\n",
      " '\\n'\n",
      " '3. **총 보상 휴가**\\n'\n",
      " '   - 총 보상 휴가 시간: **2.00시간**\\n'\n",
      " '\\n'\n",
      " '이 정보를 기반으로 추가적인 질문이 있으시면 알려주세요!')\n"
     ]
    }
   ],
   "execution_count": 147
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
