{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:25.574836Z",
     "start_time": "2025-06-30T05:59:25.563476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os, json\n",
    "\n",
    "from textwrap import dedent\n",
    "from pprint import pprint\n",
    "\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "8a67ad672af817d5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:38.190045Z",
     "start_time": "2025-06-30T05:59:26.804221Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import uuid\n",
    "import gradio as gr"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\changgoo.kang\\PycharmProjects\\LangGraph_Agents\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3672: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:42.118097Z",
     "start_time": "2025-06-30T05:59:41.079310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('AZURE_OPENAI_VERSION')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_END_POINT')\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_KEY')\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv('DEPLOYMENT_NAME'),  # or your deployment\n",
    "    api_version=os.getenv('AZURE_OPENAI_VERSION'),  # or your api version\n",
    "    temperature=0,\n",
    ")\n"
   ],
   "id": "72ff4375d688b2de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:45.676443Z",
     "start_time": "2025-06-30T05:59:45.670096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_system_task = \"\"\"Your job is to gather information from the user about the User Story they need to create.\n",
    "\n",
    "You should obtain the following information from them:\n",
    "\n",
    "- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.\n",
    "- Success criteria the success criteria of the user story\n",
    "- Plan_of_execution: the plan of execution of the initiative\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. \n",
    "Whenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.\n",
    "Do not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. \n",
    "Always remind them that if they do not know how to answer something, you can help them.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\""
   ],
   "id": "ae90a6b891e69865",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:48.253289Z",
     "start_time": "2025-06-30T05:59:48.246438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class UserStoryCriteria(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "    objective: str\n",
    "    success_criteria: str\n",
    "    plan_of_execution: str\n",
    "\n",
    "\n",
    "llm_with_tool = llm.bind_tools([UserStoryCriteria])\n",
    "\n",
    "\n",
    "class StateSchema(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    created_user_story: bool"
   ],
   "id": "5c0c783b65ed6576",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:52.250809Z",
     "start_time": "2025-06-30T05:59:52.234049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define graph flow\n",
    "workflow = StateGraph(StateSchema)\n",
    "\n",
    "# define for node 1\n",
    "def domain_state_tracker(messages):\n",
    "    return [SystemMessage(content=prompt_system_task)] + messages\n",
    "\n",
    "# define node 1\n",
    "def call_llm(state: StateSchema):\n",
    "    \"\"\"\n",
    "    talk_to_user node function, adds the prompt_system_task to the messages,\n",
    "    calls the LLM and returns the response\n",
    "    \"\"\"\n",
    "    messages = domain_state_tracker(state[\"messages\"])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# define node 2\n",
    "def finalize_dialogue(state: StateSchema):\n",
    "    \"\"\"\n",
    "    Add a tool message to the history so the graph can see that it`s time to create the user story\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# define for node 3\n",
    "prompt_generate_user_story = \"\"\"Based on the following requirements, write a good user story in korean language:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "# define for node 3\n",
    "def build_prompt_to_generate_user_story(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, AIMessage) and m.tool_calls: #tool_calls is from the OpenAI API\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_generate_user_story.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "# define node 3\n",
    "def call_model_to_generate_user_story(state):\n",
    "    messages = build_prompt_to_generate_user_story(state[\"messages\"])\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# define conditional edge\n",
    "def define_next_action(state) -> Literal[\"finalize_dialogue\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"finalize_dialogue\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# add nodes\n",
    "workflow.add_node(\"talk_to_user\", call_llm)\n",
    "workflow.add_node(\"finalize_dialogue\", finalize_dialogue)\n",
    "workflow.add_node(\"create_user_story\", call_model_to_generate_user_story)\n",
    "\n",
    "# add edges\n",
    "workflow.add_edge(START, \"talk_to_user\")\n",
    "workflow.add_conditional_edges(\"talk_to_user\", define_next_action)\n",
    "workflow.add_edge(\"finalize_dialogue\", \"create_user_story\")\n",
    "workflow.add_edge(\"create_user_story\", END)\n"
   ],
   "id": "5353f6dd0ae0aa80",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1ecbf2fb950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:57.569133Z",
     "start_time": "2025-06-30T05:59:57.563229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory = MemorySaver()\n",
    "graph_memory = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n"
   ],
   "id": "d49d3c16fae3893a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T05:59:59.860997Z",
     "start_time": "2025-06-30T05:59:59.663209Z"
    }
   },
   "cell_type": "code",
   "source": [
    " #답변 메시지 처리를 위한 함수\n",
    "def process_message(message: str, history: List[Tuple[str, str]], thread_id: str) -> str:\n",
    "    try:\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        inputs = {\"messages\": [HumanMessage(content=message)]}\n",
    "        \n",
    "        result = graph_memory.invoke(inputs, config=config)\n",
    "        \n",
    "        if \"messages\" in result:\n",
    "            # 메시지 로깅 (선택사항)\n",
    "            print(f\"스레드 ID: {thread_id}\")\n",
    "            for msg in result[\"messages\"]:\n",
    "                msg.pretty_print()\n",
    "\n",
    "            last_message = result[\"messages\"][-1]\n",
    "            if isinstance(last_message, AIMessage):\n",
    "                return last_message.content\n",
    "\n",
    "        return \"응답을 생성하지 못했습니다.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return \"죄송합니다. 응답을 생성하는 동안 오류가 발생했습니다. 다시 시도해 주세요.\"\n",
    "\n",
    "\n",
    "# 챗봇 클래스 생성\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:\n",
    "        print(f\"Thread ID: {self.thread_id}\")\n",
    "        response = process_message(message, history, self.thread_id)\n",
    "        return response\n",
    "\n",
    "chatbot = ChatBot()\n",
    "\n",
    "# 예시 질문들\n",
    "example_questions = [\n",
    "    \"목표를 설정을 도와줘\",\n",
    "    \"어떻게 요청하면 될까?\",\n",
    "]\n",
    "\n",
    "# ChatInterface 생성\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot.chat,\n",
    "    title=\"USER STORY AI ASSISTANT\",\n",
    "    description=\"개발자를 위한 위한 유저 스토리 생성을 도와 드려요. \",\n",
    "    examples=example_questions,\n",
    "    theme=gr.themes.Soft()\n",
    ")\n"
   ],
   "id": "a3b5e8a1b00806cd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:00:04.525826Z",
     "start_time": "2025-06-30T06:00:04.039091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradio 앱 실행\n",
    "demo.launch()"
   ],
   "id": "28a1c6247146e3a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:01:13.452749Z",
     "start_time": "2025-06-30T06:01:13.327113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데모 종료\n",
    "demo.close()"
   ],
   "id": "8f5b4aa6fe98d39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
