{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:35:20.606072Z",
     "start_time": "2025-04-11T08:35:20.586603Z"
    }
   },
   "cell_type": "code",
   "source": "#https://github.com/dmesquita/task_oriented_dialogue_system_langgraph",
   "id": "dab397b975c614bb",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3870494871.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    https://github.com/dmesquita/task_oriented_dialogue_system_langgraph\u001B[0m\n\u001B[1;37m          ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#https://medium.com/@mesquitadeh",
   "id": "16c3c201259ab67a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel"
   ],
   "id": "c8608ff132ecfb56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class StateSchema(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "while True:\n",
    "    user = input(\"User (q/Q to quit): \")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")"
   ],
   "id": "a493d1eafa12c48d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt_system_task = \"\"\"Your job is to gather information from the user about the User Story they need to create.\n",
    "\n",
    "You should obtain the following information from them:\n",
    "\n",
    "- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.\n",
    "- Success criteria the sucess criteria of the user story\n",
    "- Plan_of_execution: the plan of execution of the initiative\n",
    "- Deliverables: the deliverables of the initiative\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. \n",
    "Whenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.\n",
    "Do not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. \n",
    "Always remind them that if they do not know how to answer something, you can help them.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\""
   ],
   "id": "15b37d07d35df86b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def domain_state_tracker(messages):\n",
    "    return [SystemMessage(content=prompt_system_task)] + messages"
   ],
   "id": "bba8acc54ad5c17c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from typing import List, Literal, Annotated\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "llm = AzureChatOpenAI(azure_deployment=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "                    openai_api_version=\"2023-09-01-preview\",\n",
    "                    openai_api_type=\"azure\",\n",
    "                    openai_api_key=os.environ.get('AZURE_OPENAI_API_KEY'),\n",
    "                    azure_endpoint=os.environ.get('AZURE_OPENAI_ENDPOINT'),\n",
    "                    temperature=0)\n",
    "\n",
    "prompt_system_task = \"\"\"Your job is to gather information from the user about the User Story they need to create.\n",
    "\n",
    "You should obtain the following information from them:\n",
    "\n",
    "- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.\n",
    "- Success criteria the sucess criteria of the user story\n",
    "- Plan_of_execution: the plan of execution of the initiative\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. \n",
    "Whenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.\n",
    "Do not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. \n",
    "Always remind them that if they do not know how to answer something, you can help them.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "\n",
    "class UserStoryCriteria(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "    objective: str\n",
    "    success_criteria: str\n",
    "    plan_of_execution: str\n",
    "\n",
    "llm_with_tool = llm.bind_tools([UserStoryCriteria])"
   ],
   "id": "af9bd26261e6a31c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class StateSchema(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "workflow = StateGraph(StateSchema)"
   ],
   "id": "65e448dbf8d727be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def domain_state_tracker(messages):\n",
    "    return [SystemMessage(content=prompt_system_task)] + messages\n",
    "\n",
    "def call_llm(state: StateSchema):\n",
    "    \"\"\"\n",
    "    talk_to_user node function, adds the prompt_system_task to the messages,\n",
    "    calls the LLM and returns the response\n",
    "    \"\"\"\n",
    "    messages = domain_state_tracker(state[\"messages\"])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ],
   "id": "48bc8aa666671a08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "workflow.add_node(\"talk_to_user\", call_llm)",
   "id": "c10fd20f667e50af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "workflow.add_edge(START, \"talk_to_user\")",
   "id": "8a6fd62755d8224b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def finalize_dialogue(state: StateSchema):\n",
    "    \"\"\"\n",
    "    Add a tool message to the history so the graph can see that it`s time to create the user story\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "workflow.add_node(\"finalize_dialogue\", finalize_dialogue)"
   ],
   "id": "b9f857084c3690f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt_generate_user_story = \"\"\"Based on the following requirements, write a good user story:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "def build_prompt_to_generate_user_story(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, AIMessage) and m.tool_calls: #tool_calls is from the OpenAI API\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_generate_user_story.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "def call_model_to_generate_user_story(state):\n",
    "    messages = build_prompt_to_generate_user_story(state[\"messages\"])\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_node(\"create_user_story\", call_model_to_generate_user_story)"
   ],
   "id": "f0296b8c89eb116f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def define_next_action(state) -> Literal[\"finalize_dialogue\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"finalize_dialogue\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "workflow.add_conditional_edges(\"talk_to_user\", define_next_action)"
   ],
   "id": "12540b0e52d71dcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "workflow.add_edge(\"finalize_dialogue\", \"create_user_story\")\n",
    "workflow.add_edge(\"create_user_story\", END)"
   ],
   "id": "34cd5ccc623a5cdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "while True:\n",
    "    user = input(\"User (q/Q to quit): \")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"create_user_story\" in output:\n",
    "        print(\"User story created!\")"
   ],
   "id": "3e8f894d69bcb23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d8a6949db2b691b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
