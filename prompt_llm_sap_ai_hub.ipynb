{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7dd8280d75e8f51",
   "metadata": {},
   "source": [
    "```\n",
    "Generative AI with SAP AI Core - https://developers.sap.com/group.sap-ai-core-generative.html\n",
    "\n",
    "setx AICORE_HOME \"C:\\full\\path\\to\\aicore\"\n",
    "\n",
    "setx AICORE_HOME \"E:\\LangGraph_Agents\\aicore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b25caa62e719c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T04:35:37.753372Z",
     "start_time": "2025-06-11T04:35:37.737208Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb2d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('AZURE_OPENAI_VERSION')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_END_POINT')\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9226090a400fe780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T06:15:44.894671Z",
     "start_time": "2025-06-11T06:15:44.862743Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m.getenv(\u001b[33m'\u001b[39m\u001b[33mAICORE_HOME\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault_value\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(os.getenv('AICORE_HOME', 'default_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb77bcd6a81772c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T06:15:45.917913Z",
     "start_time": "2025-06-11T06:15:45.911706Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 환경 변수에서 경로 및 프로파일 읽어오기\n",
    "# aicore_home = os.getenv('AICORE_HOME')\n",
    "# profile = os.getenv('AICORE_PROFILE', 'default')\n",
    "# config_path = os.path.join(aicore_home, f\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c4d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4735a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dsg-ai.authentication.jp10.hana.ondemand.com/oauth/token\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('AICORE_AUTH_URL', 'default_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5e4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Core client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# 클라이언트 초기화\n",
    "ai_core_client = AICoreV2Client(\n",
    "    base_url=os.getenv('AICORE_BASE_URL'), # 환경 변수에서 기본 URL 읽기\n",
    "    auth_url=os.getenv('AICORE_AUTH_URL'), # 환경 변수에서 인증 URL 읽기\n",
    "    client_id=os.getenv('AICORE_CLIENT_ID'), # 환경 변수에서 클라이언트 ID 읽기\n",
    "    client_secret=os.getenv('AICORE_CLIENT_SECRET') # 환경 변수에서 클라이언트 비밀 읽기 \n",
    ")\n",
    "\n",
    "print(\"AI Core client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613dd0adf325907b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T06:15:49.431425Z",
     "start_time": "2025-06-11T06:15:47.635956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Core client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "# 구성 파일에서 값 읽어오기\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# 클라이언트 초기화\n",
    "ai_core_client = AICoreV2Client(\n",
    "    base_url=config['AICORE_BASE_URL'],\n",
    "    auth_url=config['AICORE_AUTH_URL'],\n",
    "    client_id=config['AICORE_CLIENT_ID'],\n",
    "    client_secret=config['AICORE_CLIENT_SECRET']\n",
    ")\n",
    "\n",
    "print(\"AI Core client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3447fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AIAPIInvalidRequestException",
     "evalue": "Failed to get /deployments: Invalid Request, Missing header parameter 'AI-Resource-Group' \n Status Code: 400, Request ID:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAIAPIInvalidRequestException\u001b[39m              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model_list=\u001b[43mai_core_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeployment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(model_list)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\ai_api_client_sdk\\resource_clients\\deployment_client.py:178\u001b[39m, in \u001b[36mDeploymentClient.query\u001b[39m\u001b[34m(self, scenario_id, configuration_id, executable_ids, status, top, skip, resource_group)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Queries the deployments.\u001b[39;00m\n\u001b[32m    149\u001b[39m \n\u001b[32m    150\u001b[39m \u001b[33;03m:param scenario_id: ID of the scenario the deployments should belong to, defaults to None\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    173\u001b[39m \u001b[33;03m:rtype: class:`ai_api_client_sdk.models.deployment_query_response.DeploymentQueryResponse`\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    175\u001b[39m params = \u001b[38;5;28mself\u001b[39m._form_query_params(scenario_id=scenario_id, configuration_id=configuration_id,\n\u001b[32m    176\u001b[39m                                  executable_ids=executable_ids, status=status.value \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    177\u001b[39m                                  top=top, skip=skip)\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrest_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/deployments\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DeploymentQueryResponse.from_dict(response_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\ai_api_client_sdk\\helpers\\rest_client.py:196\u001b[39m, in \u001b[36mRestClient.get\u001b[39m\u001b[34m(self, path, params, headers, resource_group, return_bytes_content, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, params: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    167\u001b[39m         resource_group: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, return_bytes_content: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs) -> Union[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    168\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a GET request to the server.\u001b[39;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;03m    :param path: path of the endpoint the request should be sent to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \u001b[33;03m    :rtype: Union[dict, int]\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_bytes_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_bytes_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\ai_api_client_sdk\\helpers\\rest_client.py:104\u001b[39m, in \u001b[36mRestClient._handle_request\u001b[39m\u001b[34m(self, method, path, params, body_json, headers, resource_group, return_bytes_content, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     response_json = response.text\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(response_json) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response_json:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_ai_api_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code // \u001b[32m100\u001b[39m != \u001b[32m2\u001b[39m:\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AIAPIServerException(description=error_description, error_message=response.text,\n\u001b[32m    107\u001b[39m                                status_code=response.status_code)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\ai_api_client_sdk\\helpers\\rest_client.py:121\u001b[39m, in \u001b[36mRestClient.raise_ai_api_exception\u001b[39m\u001b[34m(error_description, response, response_json)\u001b[39m\n\u001b[32m    119\u001b[39m error_details = response_json[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mdetails\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status_code == \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AIAPIInvalidRequestException(description=error_description, error_message=error_message,\n\u001b[32m    122\u001b[39m                                        error_code=error_code, request_id=request_id, details=error_details)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m status_code == \u001b[32m404\u001b[39m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AIAPINotFoundException(description=error_description, error_message=error_message,\n\u001b[32m    125\u001b[39m                                  error_code=error_code, request_id=request_id, details=error_details)\n",
      "\u001b[31mAIAPIInvalidRequestException\u001b[39m: Failed to get /deployments: Invalid Request, Missing header parameter 'AI-Resource-Group' \n Status Code: 400, Request ID:"
     ]
    }
   ],
   "source": [
    "model_list=ai_core_client.deployment\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ee67b2f845ca08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:37:44.295536Z",
     "start_time": "2025-04-25T03:37:44.271647Z"
    }
   },
   "outputs": [],
   "source": [
    "model_list=['gpt-4o-mini', 'anthropic--claude-3.5-sonnet', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e506fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BoWKXoO9cOils1RGLEyt5I9XkpBdg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Yes, several Azure Cognitive Services support customer-managed keys (CMK) in Azure. This feature allows users to have more control over encryption keys used to protect their data. Some of the Azure Cognitive Services that support customer managed keys include:\\n\\n- Azure Cognitive Search\\n- Azure Cognitive Services for Vision (e.g., Face, Computer Vision)\\n- Azure Speech Service\\n- Azure Text Analytics\\n- Azure Translator\\n\\nIt's always a good idea to check the Azure documentation for the most up-to-date and specific details about which services support customer managed keys and how to implement them, as Azure continuously updates its offerings and features.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1751380377, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_efad92c60b', usage=CompletionUsage(completion_tokens=123, prompt_tokens=55, total_tokens=178, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'prompt_index': 0}])\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Do other Azure Cognitive Services support this too?\"}]\n",
    "\n",
    "kwargs = dict(model_name='gpt-4o-mini', messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49ba032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 플랭킹을 좋아해요.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "\n",
    "proxy_client = get_proxy_client('gen-ai-hub')\n",
    "\n",
    "chat_llm = ChatOpenAI(proxy_model_name='gpt-4o-mini', proxy_client=proxy_client)\n",
    "template = 'You are a helpful assistant that translates english to korean.'\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "example_human = HumanMessagePromptTemplate.from_template('Hi')\n",
    "example_ai = AIMessagePromptTemplate.from_template('Ahoy!')\n",
    "human_template = '{text}'\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "\n",
    "chain = chat_prompt | chat_llm\n",
    "\n",
    "response = chain.invoke({'text': 'I love planking.'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ec39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7719ba3075bdf93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:30:31.926897Z",
     "start_time": "2025-04-25T03:30:22.688750Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Model 'gpt-4o-mini' is not available!\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgen_ai_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproxy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minit_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_llm\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm = \u001b[43minit_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m response = llm.invoke(\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini의 장점과 단점은 무엇인지 중학생이 이해할 정도로 쉽게 50자 안에서 대답해?\u001b[39m\u001b[33m'\u001b[39m).content \n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mResponse:\u001b[39m\u001b[33m'\u001b[39m, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:237\u001b[39m, in \u001b[36minit_llm\u001b[39m\u001b[34m(proxy_client, temperature, max_tokens, top_k, top_p, init_func, model_id, *args, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_id:\n\u001b[32m    236\u001b[39m     model_kwargs[\u001b[33m'\u001b[39m\u001b[33mmodel_id\u001b[39m\u001b[33m'\u001b[39m] = model_id\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mModelType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m                   \u001b[49m\u001b[43minit_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:190\u001b[39m, in \u001b[36m_init_model\u001b[39m\u001b[34m(proxy_client, model_type, args, kwargs, init_func, model_kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m init_func:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _init_custom_model(proxy_client=proxy_client, init_func=init_func, args=args, kwargs=kwargs,\n\u001b[32m    189\u001b[39m                               model_kwargs=model_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m retrieval_result: RetrievalResult = \u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retrieval_result.registry_entry.init_func(proxy_client=retrieval_result.proxy_client,\n\u001b[32m    195\u001b[39m                                                  deployment=retrieval_result.deployment,\n\u001b[32m    196\u001b[39m                                                  **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:128\u001b[39m, in \u001b[36mCatalog.retrieve\u001b[39m\u001b[34m(self, proxy_client, args, kwargs, model_type)\u001b[39m\n\u001b[32m    125\u001b[39m proxy_client = proxy_client \u001b[38;5;129;01mor\u001b[39;00m get_proxy_client()\n\u001b[32m    126\u001b[39m model_name, model_identification_kwargs, kwargs = handle_model_args_kwargs(proxy_client=proxy_client,\n\u001b[32m    127\u001b[39m                                                                            args=args, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m registry_entry = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_registry_entry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m f_select_deployment = registry_entry.f_select_deployment \u001b[38;5;129;01mor\u001b[39;00m default_f_select_deployment\n\u001b[32m    134\u001b[39m deployment = f_select_deployment(proxy_client, **model_identification_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\LangGraph_Agents\\myenv\\Lib\\site-packages\\gen_ai_hub\\proxy\\langchain\\init_models.py:118\u001b[39m, in \u001b[36mCatalog._get_registry_entry\u001b[39m\u001b[34m(self, proxy_client, model_name, only_available, model_type)\u001b[39m\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    117\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not available!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"Model 'gpt-4o-mini' is not available!\""
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "llm = init_llm(model_name='gpt-4o-mini', temperature=0.0, max_tokens=256)\n",
    "response = llm.invoke('gpt-4o-mini의 장점과 단점은 무엇인지 중학생이 이해할 정도로 쉽게 50자 안에서 대답해?').content \n",
    "print('Response:', response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846a87605e879d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:43:32.769441Z",
     "start_time": "2025-04-25T01:43:28.545222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BoW4KTkeboxjyWkL09JttLbXhHyk9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"As of my last update in October 2023, SAP's AI offerings, including those integrated with applications like SAP AI Core and SAP AI Foundation, are designed to enhance business processes and decision-making. However, specific information regarding the use of customer prompts and LLM (Large Language Model) responses can vary based on SAP's policies and offerings.\\n\\nTypically, in enterprise software environments like SAP:\\n\\n1. **Data Privacy and Security**: Customer prompts and interactions are generally subject to strict data privacy and security protocols. SAP would aim to protect customer data and ensure compliance with regulations such as GDPR.\\n\\n2. **Model Training**: If SAP uses customer prompts and AI interactions for continuous improvement of its models, this would likely be done in a way that anonymizes data to protect customer identities and proprietary information.\\n\\n3. **Feedback Loops**: Many AI models thrive on user interaction data to improve their performance. Companies, including SAP, may gather insights from user interactions to refine and enhance their AI capabilities over time.\\n\\nFor detailed and specific information about how SAP handles customer data, prompts, and AI model training, referring to official SAP documentation or reaching out to SAP support would be the best course of action. They can provide the most up-to-date and relevant information regarding data usage policies and AI functionalities.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1751379372, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_57db37749c', usage=CompletionUsage(completion_tokens=260, prompt_tokens=35, total_tokens=295, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}, 'prompt_index': 0}])\n"
     ]
    }
   ],
   "source": [
    "# open ai native\n",
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a SAP helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Does SAP use customer's prompts and llm responses of sap ai core foundation models? \"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Do other Azure Cognitive Services support this too?\"}\n",
    "]\n",
    "kwargs = dict(model_name='gpt-4o-mini', messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "147fac9c6b5b2151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:50:18.885991Z",
     "start_time": "2025-04-25T01:50:18.875011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: As of my last update in October 2023, SAP's AI offerings, including those integrated with applications like SAP AI Core and SAP AI Foundation, are designed to enhance business processes and decision-making. However, specific information regarding the use of customer prompts and LLM (Large Language Model) responses can vary based on SAP's policies and offerings.\n",
      "\n",
      "Typically, in enterprise software environments like SAP:\n",
      "\n",
      "1. **Data Privacy and Security**: Customer prompts and interactions are generally subject to strict data privacy and security protocols. SAP would aim to protect customer data and ensure compliance with regulations such as GDPR.\n",
      "\n",
      "2. **Model Training**: If SAP uses customer prompts and AI interactions for continuous improvement of its models, this would likely be done in a way that anonymizes data to protect customer identities and proprietary information.\n",
      "\n",
      "3. **Feedback Loops**: Many AI models thrive on user interaction data to improve their performance. Companies, including SAP, may gather insights from user interactions to refine and enhance their AI capabilities over time.\n",
      "\n",
      "For detailed and specific information about how SAP handles customer data, prompts, and AI model training, referring to official SAP documentation or reaching out to SAP support would be the best course of action. They can provide the most up-to-date and relevant information regarding data usage policies and AI functionalities.\n"
     ]
    }
   ],
   "source": [
    "content_value = response.choices[0].message.content\n",
    "print('Content:', content_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d40db57061f83977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:40:35.385540Z",
     "start_time": "2025-04-25T03:40:34.412535Z"
    }
   },
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "response = embeddings.create(\n",
    "    input=\"Every decoding is another encoding.\",\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    encoding_format='base64'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295bae576d1ba9d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T03:40:48.676637Z",
     "start_time": "2025-04-25T03:40:48.665095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateEmbeddingResponse(data=[Embedding(embedding='eONavIDxOzteo7I8ZBWivCz9sTspFNy77oWqu29KN7zfGLK7PGgBvWDdvjtO6dU83Y8YPO4jxbuJKHq8XLizvFhEmzzCoCY8JFG2PKKfAb3tmAK96q8sPJrzBby3HAS9JU+NvNgbADwlAIA7Ux+QvMV4zTrgVOe7I2QOPFocQrx449o65CrlvIS0Ybwu6LC8dVpBvBHSCr3bQrQ7f1Ohu37zZLqftqu7Ysg9vBHSCrtQNrq85CplPOSMSjtS0iu9xsWxvMk78ztAjQw9SHfmu5O/9LydVu+5D4UmvJeoyjxqOIS8VVmcOwmxUTxOS7u8SYgVPFWXertDx5i8gymfPDcf77pVl3q8WW14PG6bbbs3Mse6saoUPB1UBD3R+kY80frGvFRbRTsFPbk8Wi+avDu5Nzx5Qxe8x2PMPAoRDrw1NPA80au5vAWMRryVvUs959eFPOcmkzoDju+8S3MUPbRvY7uo7Wm8DiVqPMw3oTzFZfU8wo1OPCphQLxP+gS89jEmPO4jRT0H2ao7pgJrOOfELbxEoeg78F3Ru9BvBLtgoQk8Wi8aPH6R/7oumSM87Ok4OwAYLry2C9U66q8sPeNQFT3VMiq91wpRPLnj+zux+aE5oY5SPE5Lu7tv6NG771/6PGDwlrxVu4G72pNqvHrOWTxzgho7ODCeOiUAgLyUMom7h/8cPAZ57jxBerS72fXPO1prTzwGO5C86v45PSVPDTz9Llg89uIYPN96l7yfo9M8Hz+DO51px7vLJnK8MtEGvPG9DTwmPLU6rXKxOyVPDbvE7Yq8oz0cPBbR5buXlXI8j+n2vIV2Az3zNfg84RaJPLwbXzzak2q8PqINPOnCBLy4RWE8DiVqvO6Fqrw5zjg9xIulPPYxprs0qS08AQVWPLGqFLzMN6G8gitIukCNjLz8kL08uKdGPNa9bDto6x88YKGJPG9KN717G768JU8NPKmcs7sO5ws9ueN7PCz9sbz/GVc8tm06Os/8b7xxl5s8x2PMO2QVojoYWv88sEpYvMC1JzsF7iu/D3LOvLa8xzwybyG9eJRNPCB7ODzCjU48HfKeuIp1Xryz5CA8XFZOPA4larwksxs89h5OvNuRwbsN6bQ5cyC1O84ioLzlO5Q6A45vPA4larzKrgc9b0o3vHPRJ7yMEVC8KbL2O51WbzyBja284RYJvWo4hLwJxKm85Mh/uY4PJzmpnLO8f6IuPb8GXjxwSI48lrsiPIwR0Dt1qU48SChZvHO+TzuLNwA91EWCuc0RcbyWCjA8jvzOu+3njzy9ufm7xcfavOI/ZjtazTQ78F3Ru/Ug9ztswR08FOZmPAhk7Txxlxu8jNUaPPTkQbz8oxW8Gro7u/4/B7w3gdQ7x2PMuhbR5bsKT2y70qmQvCK1RDnwIZy7Hxt8PMyGLjzpYJ+8YKGJvCaLwjtDtEC83cvNPOkRkrybfsg8fga9POrrYbzoAGO8X0FNvfM1eDwIJo88mUQ8vARQEbzGJxe8xClAPbWAkjyk2Q08923bO6cTmrwxIr28YKEJvPppiTyhoaq67TadPJtrcDw89Ww7s5WTuolMgbx7zLA8WW14OxcxojylFcM6Mr6uvIRl1DsS++c81n8OvAAYrrnJO/O8UCNiO2RkrzzABDW874MBvfn29DxTH5C7SHfmOxySYrz/e7w8MV5yPNvgzrzIsDA8/FSIPBEOQDzGFD+8LBCKvDeUrLk1llW6Mr6uux4uVLuQqxg820I0O3odZzvwrN671OOcPPG9jbxgP6Q7Gc0TvOG0o7z1IPe5Boqdu4UUnryyIv+7NTTwuGJ5MLzcfmm8CWJEPA3pNLyx+SG78L+2vMquh7wyvi48gty6OxbRZbvqTcc8/3u8uiud9byBja28BjuQPI5etDypnLO8nmcePIV2A7xDtEC88qq1OnNvwjxtrsW8C5zQO+Qq5Tu3+Hy80+VFPBgeSrwiF6o8xnYkPSolC7twhuy6KNgmPNqTarz8QTA8ZHcHPKKfATvIYaM7KIkZue/SDj0sriQ75Cplu1WXervO05I7lDKJPO10+zwYHkq8T+esOLZtujsO5ws8bqwcvI9L3LySR4q6VpVRPLRvY7pVCo+8vwZevLUxBbx3p6U7Wn6nvNmmQrvgtky8qtjoOz1VqTz2Hk47e8ywvL7KKDwWgti7Tet+u1S9Kj2dVu+8QclBPQ7WXDvMN6G8TQ+GvBL757w7aqo885ddPBaC2LvvX3o8Y2bYu7RvYzyUgRY8/iwvvZ5nHrzySNA7IHs4PE/WfTwLTcM8l6hKvBu4kjz4a7I7ujDgPMsmcjwbB6A74AVaPL+keDzmd8k7Eqzau74ZNjwiFyo94j/mO1aVUb29ufk8XBoZvIUUHjxtECu82bkavArCgDsCZRI6j0vcPD3zQzzkKuU8VbsBPXsuFrya4C084RaJPE0PhjxpJ9U7JzqMPLI1V7x449q8eDLoO8Q8mDwEUJG8CbFRvE5Lu7rXbLa7BFCRPKLujjwzq1a8QmfcOlNunTxmT648YSxMPBl+hrxSIbk8YwTzO3W8prnjn6K857P+vGR3hzt449q7G7iSO7qSxbwumaM8mYDxO8yGrrxuXY+8VVmcu6GO0jygtIK7WeCMvOkRkjyzM646IMpFPAWMxrvRqzk89SD3PMmd2Dxnnju8U9CCO4IryDsoiZm8On2CPHMgNTzYqOu88KxeuZmAcbuIO9I7nbjUvAnEqbvl7IY8ESEYPAcoODst6lk8oEFuOzIgFD39Llg9/S7YPKbEjLwbRf67fga9vCphwLu0INa8oz2cvOI/ZrzwcKm8FoLYu6bEDLz095m7fAjmu58FObnQvpE7Jjw1u1W7gbxwhuw7EdIKvSY8tbwgaOA7ujDgO0O0wDrb4E68ShNYPFQMODyKOak8GwegO2ME87yeyQM8t2uRPIhOqjs3lKw8vwZevC9zczzwXVG7oz2cuiJT3zyeGBG8HpA5POTbVzs44ZA65YqhuniUzTzEiyW8zJkGvQnEqTyS5SQ9NTRwvBcxIrxEUlu8acVvvE1eE7wcQ9U8BuyCvOZ3Sbx6Hec7mAiHvHPRJzywmeW7UtIrui/VWDy/Bt68YKEJPCphQLwLryi8HaORvP8ZV7xZbXg8oiztu996l7wuNz68F4Cvu3od57w0WqA8YPAWPOqvLL0VqIg7HEPVOyxMv7wfP4O8pXeovCkU3LxU+d+8yq6HuquHsrzKTKK7d6elvOr+ubv4foo8kyFaPITHubzmFWQ7G1YtPHf2sjufBTk8t7qePH1oorvr/BC8UNRUPMk78zu6kkW8QMvqu8VldTwKT2y8IBlTO3HmqDyurmY8xNqyu3VtGbwMOuu7upJFPMqbrzsbVi28SmLlO7FbB7ynAMI7bl2Pu+FSPrz095k8n6PTvI4Pp7tZbfi5uPbTu6BSHTzkjEo89PcZvGhNBbvjn6I845+ivIayOD1sI4O8r3AIO9pVDL0srqQ7JLMbOzNcSTzAZpq8trxHvESh6Ds988O7IlNfPFHB/LtBGM86iDvSvH9TIbwPNpm7DtbcO5x8nzzVH9K7GLzkOlhEG7xyM428LBCKu8TtCr0kAqm8Hn3huoLcOjwge7g8ataePF2lW7xKE1i8WDFDPPxUiLpmYga8vd2APEShaDszXEm9+gckPIGNrbtOOOO7WmtPupf3VzyrOCU9LK6kPBu4Ejxbuty8jf73O4GNLTydace7NKmtuiwQCju/Fw08sfkhPOFSvrtcGhk9wKJPPHOCGjyIO9I7itdDPQmx0TuQqxg8ChGOPFHB/DuvcAg8/7dxu1ZGxLzRqzm8OR3GO7UeLTsG7AI9Nfi6O8Pc2zoJxKk88Up5PMN69ryvv5W8qU2mvFIO4byonty7c75PvcSLJb3wcKm82BuAvMbYiTyIit+8AHoTPWCOMTzYG4C8xsUxvPG9DTyI7EQ9f7UGvJt+SDywStg8b/upvHO+z7wmi0I8PPVsPPJI0LwIJg89rdQWO6mvCzw/LdA4eTC/vHod5zuFdgO8kJjAvAaKHbqS5aS5F5OHu0rESryZRDw8KCe0u/FK+Tvl7Aa9CNeBu+/SDr1PmJ+86uvhvH4GPTzYG4C79ORBPBsHoLntmAK71n8OPKieXLyjPZy7UYMeuxAQaTsYHko8O1fSvDNcSbyrmoo8wlGZOlW7Ab0+U4C6SHfmvK+svTxQhUe7xWV1uwIWhbyHsA89yznKPADJoDyFFB68WEQbvZtr8LylZNA7EvtnPN1ACzsIZO27PGiBuyEG+7ogLKu8ChEOvBsHoLtFASU9wAS1vCK1xLstm0y8u/IBPAuvqDwodsE8cPkAvGuwbrzrS548E1ukvFiTqDx4Mmg8ZzxWvMECDLxrsO47oEHuOyEG+7ww5gc91NJtvBVGI7xbWHc8El1NPGR3B7tmYoa8q+mXu+Khyzu64VK8A/DUO9RFgrzU0u28a7BuPNL4nbuFUvw6ueP7PJzLrLzS+J27A45vvM/AOrw+U4A7hgHGvK0jpLtvmUS7d6clPFWX+rzWMIG8aocRPOBUZ7zLJvI7uQeDuxu4Ejzy+cK3FeS9u1iTqDrKroc8rYUJu+ezfjr6pb68yyZyOzEivbwkUba7Vvc2vDobnbw2RZ876ABjOrKXPLz2gLO7WDHDOptrcL23a5G7VKpSPGChCTxJdT09h/8cPAbsAjt0XOo8rMNnOy2IdDyGn+C800erPOYVZDt0XGo8ZHcHvE7p1Tu3HAS9Eb8yvLe6njsbaQW9mUQ8PFlt+DzU0m2820I0vKcAwrtAjYw4gI9WPHGXm7tFULI85yaTvOrr4TuB75I8QmfcPDeUrLzJO3O80zRTPDWW1Tz9zPI6MV5yvDWW1byZk0k8zw/Iu2t0uTkWgtg8BFCRvH3KBzyzRoY8fWgiPFf1jbyN/vc77ZgCvc0RcTq0grs8efQJPPpWMTvmxta8Nx/vPC9zc7z7Q9m7AbZIvCp0mDqWuyK7axLUPIidtzwaWFa9Y2ZYPGnF77ucLZK74QMxPERSW7sNSxq8b+jRuw1LmrsP1LM68Up5u31oorzhAzG71JSPPMgSlrxiKiM7rl9ZPr5ow7yPS9y4DYfPPJvN1bzLJvK5S3OUu9C+kbv6VjG759eFPKHwN7wYvOS7lIGWvEkmMLt+kX88M0lxvFprT7yWHYi8PaS2uy2bTDu1gBK8g4sEvBTm5ruIit+838kkPHwIZrwLTUM5eeGxPBnNkzxMYDw8jNWavIIryLybLzu7ataePFCFx7ztdPs7sEpYPMQ8mLwLr6g8ou6OPHBIDj3pnn07G0V+vDfjubwbRf67siL/PLlWkLtcGhm8yU5LvOd1IDxqh5E6e30jO17yPzu5BwM9+laxPMmd2Lqt1Ba8/KMVPMsmcjyGsji8hMe5Oxha/zvYqGu8vN+pPO9f+rtEoeg7AhaFvD9AqLtOmsg8m2vwvD/xmruUbj68P/GavBgeSrr095m8++FzO7inRj3GJ5c8RQElPDQLEz3BAoy8QclBvEvCobwqw6W8YSzMu2TGFL0uShY96GJIvHjjWjyrmoo7cIZsPN8riryOD6c7zIauO0JnXDymxIw8McBXu/Ugdzs7uTe8Nx/vu78GXrp/U6E89DNPPEMWJjsGee47SySHPC6Zo7utcrG7dQu0PNUyqrxFASU8FtHlvGYAoTsefWG8G2kFvG1fODwaurs7NTTwukrEyjwF21O7pXeovHdYmLx+kf+5YtuVPEaM5zu4RWG7HfKevCRRtjvOhAW7oLQCvHxqyzyQmMC8mZPJPMvqPDz5WNq7vSwOvFp+JzwxcUq8fAjmvGfa8Dxaa8+8cyC1PH+irjyk2Y06vbl5vIS04bp1C7Q8YtsVPPJIULqjjKm6rHTavAIWhbvP/G+8q5qKvIidN7xN6/47VkbEvAh1HL0Nmqc6+BwlvNpVDL1Dx5i7g9qRPDfjObyyNde6x1B0vOqvLL41NHC8yv2Uu//KybxMTWQ757N+Owz8DD3SqRA8tlrivHgy6Dx7LpY7b+hROlS9qrz+jhS8u/IBPFT537zTlji885fdO6qJ2zr8VAi7dkfpPKbEDL0ZHCG7izeAvCyuJDzZuZq8sJllvKdiJ7x4lE07TppIu/T3mbwgaOC79OTBPHp/TLxcuDO83hrbO44PJ7r2Hk488vlCvMXHWjoyb6E67ecPPLUN/rt+Bj05hHgsvOrr4bkheY88PuDru7GqlLusw2c8aJySvLOVE70fPwM8rdSWu7rh0jwSXU08WOK1O1Ose7s1ltW7FzGivI3+9zu4Cay8lDKJPIGNrbwDju+7OsyPvO7BXzu7kBw8PuDrvHn0CTo6fYK6oz0cu/oHpLv/t3G8UiE5vOOfIj3Tlri7g2d9PBi85DtJOQi8DUuau6Vk0DwdMH27O2oqPG6bbTxJdT27/d/KO3suFjyEFke8TunVu+utAz2Kdd68lmwVPAjXAbyi7g48kFwLPAbsgrsBBVa8D9SzPK+svTt59Im7fRkVPBeAL7wOJeo7TQ8GPVUKDzyvcIi8FoJYPOAF2jxjF0u8+H6KvFy4M7zsh9O5fAhmPBz0R7yEFsc8kFwLvEh3ZryXlfI6en9MPKUomz0jZI468zX4vEWyF7xaa886BuwCvTNJ8b2yhOS8q9a/PKBB7jtlAso6FagIPbUeLbz/GVc8QRhPuiPxeTyonly8xO0Kvbnje7yI7MQ6VkbEuyAsK7zGFD+8dbwmPAjXAb0yIBQ9G7gSvQrCAD1AfF07lh0IvAbsArwekLk8gymfvCaeGjxJiBU8EvvnvL57Gz2uEMy8k7/0O+I/Zrs/LdC7CNeBPM/AOrtjF8u8FfeVPLUxBbwiZre7hGXUOwFnu7ySlhe7YiqjO1rNtLvsh9O7vH1EPKMqRLzT5cW8flVKvVy4s7sL/rW8CNeBvL57GzxDA847bg6CvDZFn7s4MJ47BJ8ePB7fxrut1Ja8ABiuvGIqozwsX5c69s/AvA7nC73jn6K5YxdLuVxpJrya4K27axLUPEErp7sKwgA85IzKvHPRJzyxSK+7vBtfumKMiDxQI2K7HfIevM4ioLxe8j88l/fXvJwtkrs/LVA8KHbBOXt9Izx54TE86q+svHOCGjrAtSc8uuFSu7RvY71o6x+9ajiEPKextDvI/726PlOAPEHcGT3hZZa8PGgBvaMqxLyyl7w8VPlfvAh1nLzgtsy7Rj3au9a9bDvAU8K8mpGgOyxMvzwuN768meLWulPQgrwyIJS8j+l2POEWCTyJKPq6D3JOvI1xDLvTlji8qetAu92PmDxSIbk7lIGWu6SKADwP1LM5aOufvFnP3TzXHSm8JotCuhoJSTwqJYu8UCNivH7z5DrMhi6864n8PG2uxTugAxA80vgdPdioaz3jUJW8ztMSvVLSK7vURQK8ZrGTvDoK7rqEZdS6VQoPu1A2uru+t9A7ajiEPAhkbTxrdDm8hXaDvPOojLy/pHi8iUwBPWEZ9LsF7qs812w2uxWVMD36pb48Hxv8O8AENbz0Rqc8efQJPeMBCDw3H+87mvMFvBbRZbzMhi48j0vcPCTvUDyMczU8U9CCu+qc1DsLTUM79jGmPEbuTLxg8Ja7HEPVu7owYLwNh0+8C5zQOz9AqDtgP6Q8X5DauxAQaTxXpgA9yTtzO11DdrqGYyu7t/j8OlVZHLz/ysm7m81VOuuJ/Ds/8Zo8+M2XOzu5tzwIdRw8oY5SPFu6XLznJpO8wo3OvDl/qzwOJeq8u0EPvdlXNTySNLI8k9JMu7+k+Lux+SG8qonbO5+2K7yrmoq6ACuGuqx0WrwTDBe9KnSYvG/o0TtrJaw8o4ypPKmvC7ydVm88MV5yPAEFVjwJE7e6ZMaUPOFSPjs9QtG8oFKdPNfOm7sSrFq8BAEEvZzehLtRwXy87sFfPIzVGrzq62E9pxOaPHJxa7zoAGM8JyndOwtgmzzhAzE8XlQlPe7UNzwrnXW8nRo6PFngDDosXxc76Z79vEJnXLtxhMM7zXNWu0tzFD1H7CO8Hn3hPEK26TxRwXw7YiojPLnj+zqFUny8/PIiuvbPQD2KiLY8vhm2vL8XjTgxcUq8FJfZO6qJW70V9xW8/S7YN/+3cTpeBZg6RFJbvO/Sjjyq2Gg8U6x7PA+FJjt8udi7DUuavBTm5jy8fUS8+QlNOykU3LzTNFO8', index=0, object='embedding')], model='text-embedding-ada-002', object='list', usage=Usage(prompt_tokens=6, total_tokens=6))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92662fcfc21738d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:02:13.633643Z",
     "start_time": "2025-04-29T01:02:13.625859Z"
    }
   },
   "outputs": [],
   "source": [
    "# 환경 변수에서 경로 및 프로파일 읽어오기\n",
    "aicore_home = os.getenv('AICORE_HOME')\n",
    "hana_config_path = os.path.join(aicore_home, \"hana_config.json\")\n",
    "\n",
    "with open(hana_config_path, 'r') as hana_config_file:\n",
    "    hana_config = json.load(hana_config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40e71957a8279d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T00:36:43.388950Z",
     "start_time": "2025-04-29T00:36:43.384028Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# HANA 연결 설정\n",
    "my_hana_config = {\n",
    "    'address': hana_config['HOST'],\n",
    "    'port': hana_config['PORT'],  # 기본 포트\n",
    "    'user': hana_config['USER'],\n",
    "    'password': hana_config['PASSWORD'],\n",
    "    'encrypt': 'true',\n",
    "    'sslValidateCertificate': 'false'  # 개발 환경에서는 false, 운영은 true 권장\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65f0eddf6c2f74c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:02:17.568966Z",
     "start_time": "2025-04-29T01:02:16.913475Z"
    }
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "    # DB 연결\n",
    "    # conn = dbapi.connect(**my_hana_config)\n",
    "conn = dbapi.connect(\n",
    "    address=hana_config['HOST'],\n",
    "    port=hana_config['PORT'],\n",
    "    user=hana_config['USER'],\n",
    "    password=hana_config['PASSWORD'],\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "24e9eca8db1c7053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:06:16.799560Z",
     "start_time": "2025-04-29T01:06:16.723074Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vector Engine 활성화 확인\n",
    "cursor.execute(\"select * from tables WHERE TABLE_NAME='DOCUMENTS'\")\n",
    "result = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c7529c644465c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T01:08:03.200343Z",
     "start_time": "2025-04-29T01:08:03.189444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " column_names': ('SCHEMA_NAME', 'TABLE_NAME', 'TABLE_OID', 'COMMENTS', 'FIXED_PART_SIZE', 'IS_LOGGED', 'IS_SYSTEM_TABLE', 'IS_COLUMN_TABLE', 'TABLE_TYPE', 'IS_INSERT_ONLY', 'IS_TEMPORARY', 'TEMPORARY_TABLE_TYPE', 'COMMIT_ACTION', 'IS_USER_DEFINED_TYPE', 'HAS_PRIMARY_KEY', 'USES_EXTKEY', 'AUTO_MERGE_ON', 'USES_DIMFN_CACHE', 'IS_PUBLIC', 'AUTO_OPTIMIZE_COMPRESSION_ON', 'COMPRESSED_EXTKEY', 'HAS_TEXT_FIELDS', 'USES_QUEUE_TABLE', 'IS_PRELOAD', 'IS_PARTIAL_PRELOAD', 'UNLOAD_PRIORITY', 'IS_REPLICA', 'HAS_STRUCTURED_PRIVILEGE_CHECK', 'ROW_ORDER_TYPE', 'CREATE_TIME', 'TEMPORAL_TYPE', 'HAS_MASKED_COLUMNS', 'MASK_MODE', 'PERSISTENT_MEMORY', 'HAS_RECORD_COMMIT_TIMESTAMP', 'IS_REPLICATION_LOG_ENABLED', 'NUMA_NODE_INDEXES', 'IS_MOVABLE', 'LOAD_UNIT') \n",
      " 'column_values': ('75ECB26488F84BD0A4ECE9FDF48062C5_4KFD2M8KBKQQ8RCEDVY99XRGI_RT', 'DOCUMENTS', 275415, None, 40, 'TRUE', 'FALSE', 'TRUE', 'COLUMN', 'FALSE', 'FALSE', 'NONE', None, 'FALSE', 'TRUE', 'FALSE', 'TRUE', 'FALSE', 'TRUE', 'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 5, 'FALSE', 'FALSE', None, datetime.datetime(2025, 4, 29, 0, 58, 46, 941000), None, 'FALSE', None, None, 'FALSE', 'FALSE', None, 'TRUE', 'DEFAULT') \n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\" column_names': {result.column_names} \"\"\")\n",
    "print(f\"\"\" 'column_values': {result.column_values} \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9172adc66a0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "from hdbcli import dbapi\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/vector-check')\n",
    "def vector_check():\n",
    "    try:\n",
    "        # DB 연결\n",
    "        conn = dbapi.connect(**hana_config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Vector Engine 활성화 확인\n",
    "        cursor.execute(\"SELECT * FROM M_FEATURE_OVERVIEW WHERE FEATURE_NAME = 'Vector Engine'\")\n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        return jsonify({\n",
    "            'feature': result[0],\n",
    "            'enabled': result[1]\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ef8a5ec5eb409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
